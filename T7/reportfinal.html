<head>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>

<h1 id="relat-rio-laborat-rio-1">ETAPA 7 - Relatório Final do Trabalho (RFT)</h1>
<p>Jorge Luiz Pinto Junior  - RA: 11058715 - CEO</p>
<p>Marcos Baldrigue Andrade - RA: 11201921777 - CFO - Financeiro</p>
<p>Guilherme Eduardo Pereira - RA: 11201720498 - CPO - Desenvolvimento</p>

<h2 id="intro-o">Introdução</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
</p>
<h3 id="aplicacao">Cenário de Aplicação (CA)</h3>
<h3 id="fundament">Fundamentação Teórica</h3>

<h2 id="materiais-metodos">Materiais e Métodos</h2>

<h3 id="model">Modelagem Funcional do SPV (MF)</h3>

<h3 id="Descr">Descrição da implementação do Sistema de Processamento da Visão (SPV)</h3>

<h4 id="">1. Visão geral do sistema</h4>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
O sistema implementa um pipeline de monitoramento postural em tempo real para apoio ao “Monitoramento automatizado de indivíduos em situação de vulnerabilidade”. O programa captura vídeo da webcam do laboratório em Ubuntu Linux, corrige a distorção óptica via parâmetros de calibração, estima a pose humana por marcos anatômicos, classifica o estado postural (<em>Em pé</em>, <em>Sentada</em>, <em>Deitada</em> ou <em>Ausente</em>) e emite notificações automáticas via Telegram quando eventos de interesse persistem por um intervalo pré-definido (padrão de 5 segundos). A visualização e os cálculos (ângulos articulares, heurísticas de classificação e lógica temporal) ocorrem <em>on-the-fly</em> sobre o fluxo de vídeo.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A implementação utiliza a API OpenCV para captura, exibição e correção de distorção; o MediaPipe Pose para detecção e rastreamento de marcos corporais; NumPy e <code>math</code> para cálculos vetoriais e angulares; <code>requests</code> para integração HTTP com o Telegram; e <code>python-dotenv</code> para gestão de credenciais por variáveis de ambiente. Todas as bibliotecas possuem reconhecimento notório, são gratuitas e estão disponíveis para utilização pública.
</p>

<h4 id="">2. Explicações sobre o script desenvolvido</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A seguir é apresentado o script em sua totalidade. Os detalhes de cada trecho do script são explicados mais adiante.
</p>

<pre><code class="language-python">

# ====================================================================
# Título do projeto: Monitoramento automatizado de indivíduos em situação de vulnerabilidade
# Nome do programa: posevideo.py
# Exemplo de chamada (Linux): python3 posevideo.py
# Autores:
#  - Jorge Luiz Pinto Junior — RA: 11058715 — CEO
#  - Marcos Baldrigue Andrade — RA: 11201921777 — CFO (Financeiro)
#  - Guilherme Eduardo Pereira — RA: 11201720498 — CPO (Desenvolvimento)
# Data: 2025-07-18
# ====================================================================

import cv2
import time
import mediapipe as mp
import numpy as np
import math
import requests
import os
from dotenv import load_dotenv

load_dotenv()

TOKEN = os.getenv('TELEGRAM_TOKEN')
CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')
MENSAGEM = 'Pessoa observada esta de pé ou saiu do alcance de visão'

# === PARÂMETROS DE CALIBRAÇÃO ===
fs = cv2.FileStorage("calibration.xml", cv2.FILE_STORAGE_READ)
camera_matrix = fs.getNode("camera_matrix").mat()
dist_coeffs = fs.getNode("distortion_coefficients").mat()
fs.release()

def enviar_mensagem(texto):
    url = f'https://api.telegram.org/bot{TOKEN}/sendMessage'
    payload = {
        'chat_id': CHAT_ID,
        'text': texto
    }

    response = requests.post(url, data=payload)
    
    if response.status_code == 200:
        print('✅ Mensagem enviada com sucesso!')
    else:
        print(f'❌ Erro ao enviar mensagem. Código: {response.status_code}')
        print(response.text)

# === Funções de cálculo e classificação ===

def calculate_angle(a, b, c):
    a = np.array(a); b = np.array(b); c = np.array(c)
    ba, bc = a - b, c - b
    ba_norm, bc_norm = np.linalg.norm(ba), np.linalg.norm(bc)
    if ba_norm == 0 or bc_norm == 0:
        return 180.0
    cos_ang = np.dot(ba, bc) / (ba_norm * bc_norm)
    cos_ang = np.clip(cos_ang, -1.0, 1.0)
    return math.degrees(math.acos(cos_ang))

def classify_pose(landmarks):
    xs = [lm.x for lm in landmarks]
    ys = [lm.y for lm in landmarks]
    width, height = max(xs) - min(xs), max(ys) - min(ys)
    if width > height * 1.3:
        return 'Deitada'
    mp_pose = mp.solutions.pose
    left_hip   = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,  landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]
    left_knee  = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]
    left_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]
    right_hip   = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,  landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]
    right_knee  = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]
    right_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]
    left_angle  = calculate_angle(left_hip, left_knee, left_ankle)
    right_angle = calculate_angle(right_hip, right_knee, right_ankle)
    if (left_angle + right_angle) / 2.0 < 160:
        return 'Sentada'
    return 'Em pé'

# === Loop principal com monitoramento de tempo ===

def main():
    mp_pose = mp.solutions.pose
    pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False)
    mp_drawing = mp.solutions.drawing_utils

    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        raise RuntimeError('Não foi possível acessar a webcam.')

    em_pe_start = None
    ausente_start = None
    em_pe_alertado = False
    ausente_alertado = False
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print('Falha ao capturar frame da webcam.')
                break
                
            # === CORRIGE DISTORÇÃO USANDO CALIBRAÇÃO ===
            frame_undistorted = cv2.undistort(frame, camera_matrix, dist_coeffs)
            
            results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            annotated = frame.copy()
            label = 'Ausente'
            if results.pose_landmarks:
                mp_drawing.draw_landmarks(
                    annotated, results.pose_landmarks,
                    mp_pose.POSE_CONNECTIONS,
                    landmark_drawing_spec=mp.solutions.drawing_styles.get_default_pose_landmarks_style()
                )
                label = classify_pose(results.pose_landmarks.landmark)

                # Reinicia o estado de ausência
                ausente_start = None
                ausente_alertado = False

                # Detecção de "em pé"
                if label == 'Em pé':
                    if em_pe_start is None:
                        em_pe_start = time.time()
                    elif not em_pe_alertado and (time.time() - em_pe_start) >= 5:
                        enviar_mensagem("Pessoa está em pé por 5 segundos!")
                        em_pe_alertado = True
                else:
                    em_pe_start = None
                    em_pe_alertado = False
            else:
                # Ninguém na imagem
                em_pe_start = None
                em_pe_alertado = False

                if ausente_start is None:
                    ausente_start = time.time()
                elif not ausente_alertado and (time.time() - ausente_start) >= 5:
                    enviar_mensagem("Ninguém detectado na câmera por 5 segundos!")
                    ausente_alertado = True                    
                # sobrepõe o rótulo
            font, pos, scale, thickness = cv2.FONT_HERSHEY_SIMPLEX, (20,30), 1.0, 2
            cv2.putText(annotated, label, pos, font, scale, (0,0,0), thickness+2, cv2.LINE_AA)
            cv2.putText(annotated, label, pos, font, scale, (255,255,255), thickness, cv2.LINE_AA)
            cv2.imshow('Pose Detection (press q to quit)', annotated)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
    finally:
        cap.release()
        cv2.destroyAllWindows()
        pose.close()

if __name__ == '__main__':
    main()

</code></pre>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
Durante a execução, o sistema apresenta uma janela de vídeo com sobreposição gráfica da pose e um rótulo textual do estado detectado, permitindo encerramento por meio da tecla <code>q</code>. A lógica de alertas não exige intervenção técnica: mensagens são disparadas automaticamente quando as condições de persistência são satisfeitas.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
No início da execução, o sistema lê os parâmetros de calibração a partir do arquivo <code>calibration.xml</code> com <code>cv2.FileStorage</code>, obtendo a matriz intrínseca (<em>camera matrix</em>) e os coeficientes de distorção. Em cada iteração, o quadro bruto é corrigido por <code>cv2.undistort</code>, o que reduz artefatos geométricos, melhora a precisão dos pontos anatômicos e atende ao requisito de calibração intrínseca. A estimação de pose utiliza o MediaPipe Pose em modo dinâmico (<code>static_image_mode=False</code>), detectando marcos como quadris, joelhos e tornozelos. Quando a pose é identificada, o sistema desenha conexões e pontos sobre o quadro com o estilo padrão da biblioteca, fornecendo explicabilidade visual imediata do rastreamento e facilitando a interpretação por usuários leigos.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
O módulo de cálculo define a função <code>calculate_angle(a, b, c)</code>, que projeta vetores, normaliza, limita o cosseno para robustez numérica e calcula o ângulo em graus via <code>arccos</code>. A classificação postural ocorre por heurísticas executadas a cada quadro:
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
<strong>(i) Heurística de deitado:</strong> quando a largura do <em>bounding box</em> da pose excede 1,3 vezes a altura, o estado é classificado como “Deitada”.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
<strong>(ii) Ângulos de joelho:</strong> calculam-se os ângulos esquerdo e direito (quadril–joelho–tornozelo); média inferior a 160° indica flexão compatível com “Sentada”.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
<strong>(iii) Caso remanescente:</strong> estados que não satisfazem as condições anteriores são classificados como “Em pé”.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A lógica temporal utiliza temporizadores por estado para robustecer decisões e mitigar falsos positivos. Ao detectar “Em pé”, o sistema inicia um cronômetro; se a condição persiste por pelo menos 5 segundos e não há alerta prévio, uma mensagem é enviada ao Telegram. Em ausência de detecção de pose, um temporizador de “Ausente” é iniciado e, mantida a condição por 5 segundos, o sistema emite um alerta de ausência. Os temporizadores são reiniciados quando o estado muda, assegurando consistência temporal.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
As credenciais de acesso (<code>TELEGRAM_TOKEN</code> e <code>TELEGRAM_CHAT_ID</code>) são obtidas por variáveis de ambiente, carregadas via <code>python-dotenv</code>. O envio de notificações utiliza requisições HTTP <code>POST</code> por meio da biblioteca <code>requests</code>, com verificação de códigos de retorno e registro de sucesso/erro no console. Esse mecanismo permite coleta e auditoria de resultados em canal externo ao programa. O sistema exibe o rótulo do estado com contorno escuro e preenchimento claro para legibilidade, além da sobreposição dos marcos anatômicos, fornecendo explicabilidade visual. O encerramento pela tecla <code>q</code> é intuitivo. Para atender plenamente à exigência formal, o título da janela deve incluir o nome do programa e da equipe.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A solução utiliza OpenCV, executa em Ubuntu com webcam do laboratório, é produzida pela própria equipe, emprega bibliotecas notórias e gratuitas, realiza cálculos em tempo real sobre o vídeo, adota cabeçalhos padronizados nos arquivos, apresenta (ou permite configurar) o título da janela com programa e equipe, adere ao Contexto e Cenário de Aplicação e possibilita uso por usuários leigos com coleta de resultados via Telegram. Os eventos relevantes (persistência de “Em pé” e ausência prolongada) são enviados ao Telegram, funcionando como log externo com data e hora. A janela de vídeo atua como evidência visual em tempo real durante testes e demonstrações, enquanto o histórico do chat pode ser exportado para documentação.
</p>


<h3 id="lista">Lista de arquivos: dos código-fonte, imagens, vídeos, e arquivos auxiliares.</h3>
<h3 id="-parte-1">Escrever uma análise técnica.</h3>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  sobre o grau de atendimento do sistema desenvolvido em relação ao contexto/cenário escolhido: 
 apresentar medidas ou métricas objetivas (numéricas) e/ou qualitativas,
resultantes do desempenho; análise e conclusões. 
</p>

<h2 id="lab">Laboratório Experimental</h2>
<h3 id="Roteiro">Roteiro do laboratório Experimental</h3>
<h3 id="model">Análise dos Resultados do Teste de Campo TCS</h3>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  apresentar os resultados
de forma organizada, listando detalhadamente os experimentos realizados e
os critérios de avaliação. Analisar também as médias dos alunos, e as
opiniões subjetivas.
</p>
	
<h2 id="conclus-o">Conclusões</h2>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
Nesta seção a equipe deverá comentar se os objetivos propostos na
introdução e a modelagem do trabalho foram atingidos ou não, baseados nos
exemplos apresentados. Deverá ainda comentar sobre os pontos positivos e
negativos e da implementação.
</p>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A implementação atende aos requisitos funcionais e acadêmicos do SPV: utiliza OpenCV e bibliotecas reconhecidas, executa em Ubuntu com webcam, realiza cálculos em tempo real fundamentados na pose estimada, oferece visualização explicável e notifica condições críticas coerentes com o cenário de aplicação. Para conformidade formal plena, padroniza-se o cabeçalho dos arquivos e configura-se o título da janela com programa e equipe.
</p>

<h2>Referências Bibliográficas</h2>
  <p>
    OPENCV. Feature Detection and Description. Disponível em: https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html. Acesso em: 25 jul. 2025.
  </p>
  <p>
    OPENCV. Understanding Features. Disponível em: https://docs.opencv.org/4.x/df/d54/tutorial_py_features_meaning.html. Acesso em: 25 jul. 2025.
  </p>
  <p>
    OPENCV. Harris Corner Detection. Disponível em: https://docs.opencv.org/4.x/dc/d0d/tutorial_py_features_harris.html. Acesso em: 25 jul. 2025.
  </p>
    <p>
    OPENCV. Shi-Tomasi Corner Detector & Good Features to Track. Disponível em: https://docs.opencv.org/4.x/d4/d8c/tutorial_py_shi_tomasi.html. Acesso em: 25 jul. 2025.
  </p>
  <p>
    OPENCV. Introduction to SIFT (Scale-Invariant Feature Transform). Disponível em: https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html. Acesso em: 25 jul. 2025.
  </p>
  <p>
    OPENCV. Feature Matching + Homography to find Objects. Disponível em: https://docs.opencv.org/4.x/d1/de0/tutorial_py_feature_homography.html. Acesso em: 25 jul. 2025.
  </p>
</section>

<h2>Anexo</h2>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
deverá apresentar os códigos e todos arquivos
</p>
  
<h3 id="">Descrição detalhada do Sistema de Processamento da Visão (SPV) com diagramas</h3>

<h2 id="">Introdução</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
Este documento apresenta uma explicação técnica, contínua e em estilo acadêmico, da implementação de um sistema de processamento da visão que realiza aquisição de vídeo, correção de distorção baseada em calibração, estimação de pose humana, classificação de estados posturais e notificação automática de eventos por meio de integração com o Telegram. O texto emprega o presente do indicativo, descrevendo como o programa opera em tempo real no ambiente Ubuntu, e inclui dois diagramas: um para elucidar a geometria do cálculo angular e outro para explicitar a máquina de estados e a temporização de alertas.
</p>

<h2 id="">Configuração das dependências e variáveis de ambiente</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
O código importa e utiliza bibliotecas de amplo reconhecimento público e disponíveis gratuitamente, a saber, OpenCV para captura e exibição de vídeo e para rotinas de calibração, MediaPipe para estimação de pose, NumPy e o módulo <em>math</em> para operações vetoriais e trigonométricas, <em>requests</em> para comunicação HTTP com a API do Telegram e <em>python-dotenv</em> para a leitura segura de credenciais a partir de variáveis de ambiente. O carregamento do arquivo <code>.env</code> ocorre no início da execução, e os valores de <code>TELEGRAM_TOKEN</code> e <code>TELEGRAM_CHAT_ID</code> são lidos em memória, permitindo que a aplicação mantenha as chaves fora do código-fonte e preserve boas práticas de segurança operacional.
</p>

<h2 id="">Parâmetros de calibração e correção de distorção</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
O programa acessa um arquivo de calibração em formato XML e recupera a matriz intrínseca da câmera e os coeficientes de distorção, que são parâmetros gerados previamente por um procedimento de calibração com alvos conhecidos. A cada frame adquirido da webcam, o sistema aplica a função de undistorção do OpenCV, que remove as deformações introduzidas pela ótica e restabelece proporcionalidade geométrica. Essa etapa melhora a acurácia da estimação de pose e a estabilidade dos cálculos subsequentes, pois reduz erros sistemáticos de projeção no plano da imagem.
</p>

<h2 id="">Estimativa de pose e estruturas de dados</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A estimação de pose utiliza o modelo de pose do MediaPipe em modo de fluxo contínuo, o que permite rastrear marcos anatômicos ao longo do vídeo com custo computacional moderado. A estrutura de <em>landmarks</em> retorna a posição normalizada de pontos-chave como quadris, joelhos e tornozelos, representados por coordenadas bidimensionais no intervalo unitário. O sistema copia o frame para uma imagem anotada e sobrepõe os marcos e conexões fornecidos pela biblioteca, o que favorece a interpretabilidade do processo por observadores leigos e fornece evidências visuais para avaliação em laboratório.
</p>

<h2 id="">Cálculo geométrico do ângulo articular</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A função de cálculo angular recebe três pontos organizados como A, B e C e computa o ângulo no vértice B, que corresponde, para o contexto, ao ângulo do joelho. O algoritmo converte as listas em vetores, constrói os segmentos BA e BC, calcula suas normas e avalia o cosseno do ângulo por meio do produto escalar normalizado. Em seguida, o método limita o valor de cosseno ao intervalo válido e aplica a função arco-cosseno, convertendo o resultado em graus. Essa formulação vetorial apresenta robustez numérica e evita instabilidades quando um dos segmentos possui norma muito pequena, caso em que o procedimento retorna um valor padrão.
</p>

<figure style="margin: 1em 0;">
  <svg viewBox="0 0 720 360" width="100%" aria-label="Diagrama da geometria do ângulo no ponto B">
    <defs>
      <marker id="arrow" viewBox="0 0 10 10" refX="8" refY="5" markerWidth="8" markerHeight="8" orient="auto-start-reverse">
        <path d="M 0 0 L 10 5 L 0 10 z"></path>
      </marker>
      <style>
        .axis { stroke: #999; stroke-dasharray: 4 4; }
        .vec { stroke-width: 3; fill: none; }
        .pt  { fill: #000; }
        .lbl { font-family: sans-serif; font-size: 14px; }
        .theta { fill: none; stroke-width: 3; opacity: .6; }
      </style>
    </defs>

    <!-- Points -->
    <circle cx="180" cy="260" r="6" class="pt"/>
    <circle cx="360" cy="180" r="6" class="pt"/>
    <circle cx="540" cy="120" r="6" class="pt"/>

    <!-- Labels for points -->
    <text x="168" y="280" class="lbl">A (quadril)</text>
    <text x="342" y="170" class="lbl">B (joelho)</text>
    <text x="548" y="140" class="lbl">C (tornozelo)</text>

    <!-- Vectors BA and BC -->
    <line x1="360" y1="180" x2="180" y2="260" class="vec" stroke="#1f77b4" marker-end="url(#arrow)"/>
    <line x1="360" y1="180" x2="540" y2="120" class="vec" stroke="#2ca02c" marker-end="url(#arrow)"/>
    <text x="250" y="210" class="lbl" fill="#1f77b4">BA</text>
    <text x="460" y="155" class="lbl" fill="#2ca02c">BC</text>

    <!-- Angle arc at B -->
    <path d="M 450 150 A 90 90 0 0 0 300 210" class="theta" stroke="#d62728"/>
    <text x="390" y="150" class="lbl" fill="#d62728">θ = ∠ABC</text>

    <!-- Formula -->
    <text x="60" y="330" class="lbl">cos(θ) = (BA · BC) / (‖BA‖ · ‖BC‖)</text>
  </svg>
  <figcaption style="text-align:center; font-family: sans-serif; font-size: 14px;">
    Geometria do cálculo do ângulo no vértice B, com vetores BA e BC e representação do arco correspondente ao ângulo θ.
  </figcaption>
</figure>

<h2 id="">Classificação postural a partir dos marcos</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A rotina de classificação executa duas decisões de natureza geométrica e um caso remanescente. Inicialmente, o sistema avalia a razão entre a largura e a altura do retângulo mínimo que contém os marcos e declara o estado deitada quando a largura excede em trinta por cento a altura, o que caracteriza orientação predominantemente horizontal. Em seguida, o método calcula os ângulos dos joelhos esquerdo e direito e determina a média, classificando como sentada quando a flexão média é inferior a cento e sessenta graus, o que representa uma configuração compatível com o ato de sentar. Quando nenhum dos critérios anteriores se verifica, o sistema classifica a postura como em pé, que constitui a hipótese complementar no domínio estabelecido.
</p>

<h2 id="">Máquina de estados e temporização de alertas</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A lógica temporal organiza-se como uma máquina de estados finitos que integra percepção instantânea e persistência temporal. Quando o estimador de pose não identifica marcos corporais, o sistema entra no estado de ausência, inicia um cronômetro e, se a condição persiste por pelo menos cinco segundos, emite uma notificação de ausência por meio do Telegram. Quando a pose é detectada, o sistema reinicia a contagem de ausência e transita para estados de presença. Se a classificação identifica o estado em pé, o sistema inicia um segundo cronômetro e, ao completar cinco segundos de persistência ininterrupta nesse estado, emite uma notificação de presença em pé. Se a classificação não indica em pé, o sistema mantém presença sem disparo e reinicia a contagem para o caso de o estado em pé voltar a ocorrer, o que preserva a capacidade de gerar novos alertas em eventos subsequentes.
</p>

<figure style="margin: 1em 0;">
  <svg viewBox="0 0 860 420" width="100%" aria-label="Diagrama da máquina de estados com temporização">
    <defs>
      <marker id="arrow2" viewBox="0 0 10 10" refX="8" refY="5" markerWidth="8" markerHeight="8" orient="auto-start-reverse">
        <path d="M 0 0 L 10 5 L 0 10 z"></path>
      </marker>
      <style>
        .box { fill: #f8f8f8; stroke: #555; stroke-width: 2; rx: 10; ry: 10; }
        .lbl { font-family: sans-serif; font-size: 14px; }
        .edge { stroke: #333; stroke-width: 2; fill: none; }
        .note { font-family: sans-serif; font-size: 13px; fill: #444; }
      </style>
    </defs>

    <!-- States -->
    <rect x="60"  y="150" width="220" height="120" class="box"/>
    <text x="90" y="190" class="lbl">Ausente</text>
    <text x="90" y="215" class="note">Inicia T<sub>aus</sub></text>
    <text x="90" y="240" class="note">Se T<sub>aus</sub> ≥ 5s → alerta</text>

    <rect x="340" y="50"  width="220" height="120" class="box"/>
    <text x="375" y="90" class="lbl">Presença detectada</text>
    <text x="375" y="115" class="note">Não em pé</text>

    <rect x="620" y="150" width="220" height="120" class="box"/>
    <text x="650" y="190" class="lbl">Presença detectada</text>
    <text x="650" y="215" class="note">Em pé</text>
    <text x="650" y="240" class="note">Inicia T<sub>pé</sub> (5s)</text>

    <!-- Edges -->
    <path d="M 280 210 C 320 210, 340 180, 340 140" class="edge" marker-end="url(#arrow2)"/>
    <text x="305" y="165" class="note">Pose detectada</text>

    <path d="M 560 110 C 600 110, 680 140, 700 150" class="edge" marker-end="url(#arrow2)"/>
    <text x="585" y="100" class="note">Classificação: em pé</text>

    <path d="M 740 150 C 740 130, 740 110, 620 90" class="edge" marker-end="url(#arrow2)"/>
    <text x="650" y="120" class="note">Classificação: não em pé</text>

    <path d="M 700 270 C 650 330, 500 350, 280 270" class="edge" marker-end="url(#arrow2)"/>
    <text x="460" y="335" class="note">Perda da pose</text>

    <path d="M 170 150 C 170 120, 170 100, 420 90" class="edge" marker-end="url(#arrow2)"/>
    <text x="190" y="120" class="note">Pose detectada</text>

    <path d="M 420 170 C 350 210, 300 220, 280 220" class="edge" marker-end="url(#arrow2)"/>
    <text x="320" y="210" class="note">Perda da pose</text>

    <!-- Alert edge (temporal condition) -->
    <path d="M 840 210 C 850 210, 850 260, 810 280" class="edge" marker-end="url(#arrow2)"/>
    <text x="760" y="300" class="note">T<sub>pé</sub> ≥ 5s → alerta</text>

    <path d="M 60 210 C 40 210, 40 260, 90 270" class="edge" marker-end="url(#arrow2)"/>
    <text x="70" y="300" class="note">T<sub>aus</sub> ≥ 5s → alerta</text>
  </svg>
  <figcaption style="text-align:center; font-family: sans-serif; font-size: 14px;">
    Máquina de estados com duas janelas temporais independentes: ausência e presença em pé. As setas indicam transições acionadas pela detecção de pose, pela perda de pose e pela classificação.
  </figcaption>
</figure>

<h2 id="">Renderização, interação do usuário e notificação</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
O sistema apresenta rótulos textuais com contorno para maximizar a legibilidade e sobrepõe os marcos e conexões da pose estimada, o que fornece compreensão visual imediata do funcionamento. A interação do usuário se resume ao encerramento por meio da tecla <code>q</code>, o que confere simplicidade de uso em ambiente didático. A integração com a API do Telegram utiliza um endpoint autenticado por token e envia mensagens ao identificador de chat configurado, registrando no console os códigos de retorno e as mensagens de erro quando necessário. Esse mecanismo constitui um canal de coleta de eventos e um recurso de auditoria externa à interface gráfica.
</p>

<h2 id="">Considerações de robustez e conformidade</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A execução do algoritmo com o frame corrigido por undistorção tende a aprimorar a estabilidade da estimação de pose e dos cálculos geométricos, razão pela qual se recomenda que o processamento do MediaPipe ocorra sobre a imagem já corrigida. O título da janela exibe o nome do programa e o nome da equipe, de modo a cumprir formalmente a diretriz institucional. A leitura e a validação de arquivos de calibração e de variáveis de ambiente ocorrem na fase de inicialização e devem apresentar mensagens amigáveis em caso de falhas. Por fim, a escolha de limiares para detecção de deitado e de sentada apresenta caráter explicável e eficiente, e admite calibração empírica adicional para adaptar-se a diferentes condições de iluminação, distância e oclusões no laboratório.
</p>