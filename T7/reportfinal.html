<head>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>

<h1 id="relat-rio-laborat-rio-1">ETAPA 7 - Relatório Final do Trabalho (RFT)</h1>
<p>Jorge Luiz Pinto Junior  - RA: 11058715 - CEO</p>
<p>Marcos Baldrigue Andrade - RA: 11201921777 - CFO - Financeiro</p>
<p>Guilherme Eduardo Pereira - RA: 11201720498 - CPO - Desenvolvimento</p>

<h2 id="intro-o">Introdução</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
</p>
<h3 id="aplicacao">Cenário de Aplicação (CA)</h3>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
O Contexto e Cenário de Aplicação deste projeto de monitoramento por visão computacional, teve sua origem em entrevistas com familiares dos alunos. O problema emerge do cuidado domiciliar de uma idosa com Alzheimer e demência: a câmera instalada não emite aviso quando a pessoa se levanta, o que exige vigilância contínua e inviabiliza a prevenção durante períodos como o sono. Em 18/06/2025 registra-se a necessidade de um sistema que notifique, de forma imediata, a saída da cama, reduzindo riscos associados à locomoção desassistida.    
</p>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
    O objetivo do projeto consiste em desenvolver um sistema que identifique, em tempo real, a permanência de uma pessoa em um móvel (cama, cadeira, sofá) e que reconheça a transição de postura ao levantar, enviando um alerta imediato ao responsável. A proposta contempla o uso em contextos domésticos e institucionais, ampliando o alcance da supervisão sem requerer presença física constante do cuidador e respondendo às demandas de indivíduos em situação de vulnerabilidade.
</p>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
O funcionamento prevê uma câmera posicionada de modo estratégico, cujas imagens alimentam algoritmos de visão computacional para detecção de presença e postura; ao identificar a saída do móvel, o sistema aciona notificações por aplicativo, SMS ou sinais locais. Espera-se, como benefício, o aumento da segurança, a resposta rápida a eventos críticos e a otimização do tempo dos cuidadores, promovendo autonomia, bem-estar e qualidade de vida da pessoa assistida.
</p>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
Mais detalhes do Cenário de Aplicação podem ser obtidos através do documento de entrevistas empáticas a seguir:
</p>

<a href="/T1/Entrevistas Jorge.pdf" target="_blank" download>Download Entrevistas Jorge</a>

<h3 id="fundament">Fundamentação Teórica</h3>

<h2 id="materiais-metodos">Materiais e Métodos</h2>

<h3 id="model">Modelagem Funcional do SPV (MF)</h3>

<h3 id="Descr">Descrição da implementação do Sistema de Processamento da Visão (SPV)</h3>

<h4 id="">1. Visão geral do sistema</h4>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
O sistema implementa um pipeline de monitoramento postural em tempo real para apoio ao “Monitoramento automatizado de indivíduos em situação de vulnerabilidade”. O programa captura vídeo da webcam do laboratório em Ubuntu Linux, corrige a distorção óptica via parâmetros de calibração, estima a pose humana por marcos anatômicos, classifica o estado postural (<em>Em pé</em>, <em>Sentada</em>, <em>Deitada</em> ou <em>Ausente</em>) e emite notificações automáticas via Telegram quando eventos de interesse persistem por um intervalo pré-definido (padrão de 5 segundos). A visualização e os cálculos (ângulos articulares, heurísticas de classificação e lógica temporal) ocorrem <em>on-the-fly</em> sobre o fluxo de vídeo.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A implementação utiliza a API OpenCV para captura, exibição e correção de distorção; o MediaPipe Pose para detecção e rastreamento de marcos corporais; NumPy e <code>math</code> para cálculos vetoriais e angulares; <code>requests</code> para integração HTTP com o Telegram; e <code>python-dotenv</code> para gestão de credenciais por variáveis de ambiente. Todas as bibliotecas possuem reconhecimento notório, são gratuitas e estão disponíveis para utilização pública.
</p>

<h4 id="">2. Explicações sobre o script desenvolvido</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A seguir é apresentado o script e a descrição do seu funcionamento:
</p>

<pre><code class="language-python">

# ====================================================================
# Título do projeto: Monitoramento automatizado de indivíduos em situação de vulnerabilidade
# Nome do programa: posevideo.py
# Exemplo de chamada (Linux): python3 posevideo.py
# Autores:
#  - Jorge Luiz Pinto Junior — RA: 11058715 — CEO
#  - Marcos Baldrigue Andrade — RA: 11201921777 — CFO (Financeiro)
#  - Guilherme Eduardo Pereira — RA: 11201720498 — CPO (Desenvolvimento)
# Data: 2025-07-18
# ====================================================================

import cv2
import time
import mediapipe as mp
import numpy as np
import math
import requests
import os
from dotenv import load_dotenv

load_dotenv()

TOKEN = os.getenv('TELEGRAM_TOKEN')
CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')
MENSAGEM = 'Pessoa observada esta de pé ou saiu do alcance de visão'

# === PARÂMETROS DE CALIBRAÇÃO ===
fs = cv2.FileStorage("calibration.xml", cv2.FILE_STORAGE_READ)
camera_matrix = fs.getNode("camera_matrix").mat()
dist_coeffs = fs.getNode("distortion_coefficients").mat()
fs.release()

def enviar_mensagem(texto):
    url = f'https://api.telegram.org/bot{TOKEN}/sendMessage'
    payload = {
        'chat_id': CHAT_ID,
        'text': texto
    }

    response = requests.post(url, data=payload)
    
    if response.status_code == 200:
        print('✅ Mensagem enviada com sucesso!')
    else:
        print(f'❌ Erro ao enviar mensagem. Código: {response.status_code}')
        print(response.text)

# === Funções de cálculo e classificação ===

def calculate_angle(a, b, c):
    a = np.array(a); b = np.array(b); c = np.array(c)
    ba, bc = a - b, c - b
    ba_norm, bc_norm = np.linalg.norm(ba), np.linalg.norm(bc)
    if ba_norm == 0 or bc_norm == 0:
        return 180.0
    cos_ang = np.dot(ba, bc) / (ba_norm * bc_norm)
    cos_ang = np.clip(cos_ang, -1.0, 1.0)
    return math.degrees(math.acos(cos_ang))

def classify_pose(landmarks):
    xs = [lm.x for lm in landmarks]
    ys = [lm.y for lm in landmarks]
    width, height = max(xs) - min(xs), max(ys) - min(ys)
    if width > height * 1.3:
        return 'Deitada'
    mp_pose = mp.solutions.pose
    left_hip   = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,  landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]
    left_knee  = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]
    left_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]
    right_hip   = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x,  landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]
    right_knee  = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]
    right_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]
    left_angle  = calculate_angle(left_hip, left_knee, left_ankle)
    right_angle = calculate_angle(right_hip, right_knee, right_ankle)
    if (left_angle + right_angle) / 2.0 < 160:
        return 'Sentada'
    return 'Em pé'

# === Loop principal com monitoramento de tempo ===

def main():
    mp_pose = mp.solutions.pose
    pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False)
    mp_drawing = mp.solutions.drawing_utils

    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        raise RuntimeError('Não foi possível acessar a webcam.')

    em_pe_start = None
    ausente_start = None
    em_pe_alertado = False
    ausente_alertado = False
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print('Falha ao capturar frame da webcam.')
                break
                
            # === CORRIGE DISTORÇÃO USANDO CALIBRAÇÃO ===
            frame_undistorted = cv2.undistort(frame, camera_matrix, dist_coeffs)
            
            results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            annotated = frame.copy()
            label = 'Ausente'
            if results.pose_landmarks:
                mp_drawing.draw_landmarks(
                    annotated, results.pose_landmarks,
                    mp_pose.POSE_CONNECTIONS,
                    landmark_drawing_spec=mp.solutions.drawing_styles.get_default_pose_landmarks_style()
                )
                label = classify_pose(results.pose_landmarks.landmark)

                # Reinicia o estado de ausência
                ausente_start = None
                ausente_alertado = False

                # Detecção de "em pé"
                if label == 'Em pé':
                    if em_pe_start is None:
                        em_pe_start = time.time()
                    elif not em_pe_alertado and (time.time() - em_pe_start) >= 5:
                        enviar_mensagem("Pessoa está em pé por 5 segundos!")
                        em_pe_alertado = True
                else:
                    em_pe_start = None
                    em_pe_alertado = False
            else:
                # Ninguém na imagem
                em_pe_start = None
                em_pe_alertado = False

                if ausente_start is None:
                    ausente_start = time.time()
                elif not ausente_alertado and (time.time() - ausente_start) >= 5:
                    enviar_mensagem("Ninguém detectado na câmera por 5 segundos!")
                    ausente_alertado = True                    
                # sobrepõe o rótulo
            font, pos, scale, thickness = cv2.FONT_HERSHEY_SIMPLEX, (20,30), 1.0, 2
            cv2.putText(annotated, label, pos, font, scale, (0,0,0), thickness+2, cv2.LINE_AA)
            cv2.putText(annotated, label, pos, font, scale, (255,255,255), thickness, cv2.LINE_AA)
            cv2.imshow('Pose Detection (press q to quit)', annotated)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
    finally:
        cap.release()
        cv2.destroyAllWindows()
        pose.close()

if __name__ == '__main__':
    main()

</code></pre>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
Durante a execução, o sistema apresenta uma janela de vídeo com sobreposição gráfica da pose e um rótulo textual do estado detectado, permitindo encerramento por meio da tecla <code>q</code>. A lógica de alertas não exige intervenção técnica: mensagens são disparadas automaticamente quando as condições de persistência são satisfeitas.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
No início da execução, o sistema lê os parâmetros de calibração a partir do arquivo <code>calibration.xml</code> com <code>cv2.FileStorage</code>, obtendo a matriz intrínseca (<em>camera matrix</em>) e os coeficientes de distorção. Em cada iteração, o quadro bruto é corrigido por <code>cv2.undistort</code>, o que reduz artefatos geométricos, melhora a precisão dos pontos anatômicos e atende ao requisito de calibração intrínseca. A estimação de pose utiliza o MediaPipe Pose em modo dinâmico (<code>static_image_mode=False</code>), detectando marcos como quadris, joelhos e tornozelos. Quando a pose é identificada, o sistema desenha conexões e pontos sobre o quadro com o estilo padrão da biblioteca, fornecendo explicabilidade visual imediata do rastreamento e facilitando a interpretação por usuários leigos.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A rotina de classificação executa duas decisões de natureza geométrica e um caso remanescente. Inicialmente, o sistema avalia a razão entre a largura e a altura do retângulo mínimo que contém os marcos e declara o estado deitada quando a largura excede em trinta por cento a altura, o que caracteriza orientação predominantemente horizontal. Em seguida, o método calcula os ângulos dos joelhos esquerdo e direito e determina a média, classificando como sentada quando a flexão média é inferior a cento e sessenta graus, o que representa uma configuração compatível com o ato de sentar. Quando nenhum dos critérios anteriores se verifica, o sistema classifica a postura como em pé, que constitui a hipótese complementar no domínio estabelecido.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A função de cálculo angular recebe três pontos organizados como A, B e C e computa o ângulo no vértice B, que corresponde, para o contexto, ao ângulo do joelho. O algoritmo converte as listas em vetores, constrói os segmentos BA e BC, calcula suas normas e avalia o cosseno do ângulo por meio do produto escalar normalizado. Em seguida, o método limita o valor de cosseno ao intervalo válido e aplica a função arco-cosseno, convertendo o resultado em graus. Essa formulação vetorial apresenta robustez numérica e evita instabilidades quando um dos segmentos possui norma muito pequena, caso em que o procedimento retorna um valor padrão. A classificação postural ocorre por heurísticas executadas a cada quadro:
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
<strong>(i) Heurística de deitado:</strong> quando a largura do <em>bounding box</em> da pose excede 1,3 vezes a altura, o estado é classificado como “Deitada”.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
<strong>(ii) Ângulos de joelho:</strong> calculam-se os ângulos esquerdo e direito (quadril–joelho–tornozelo); média inferior a 160° indica flexão compatível com “Sentada”.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
<strong>(iii) Caso remanescente:</strong> estados que não satisfazem as condições anteriores são classificados como “Em pé”.
</p>

<figure style="margin: 1em 0;">
  <svg viewBox="0 0 720 360" width="100%" aria-label="Diagrama da geometria do ângulo no ponto B">
    <defs>
      <marker id="arrow" viewBox="0 0 10 10" refX="8" refY="5" markerWidth="8" markerHeight="8" orient="auto-start-reverse">
        <path d="M 0 0 L 10 5 L 0 10 z"></path>
      </marker>
      <style>
        .axis { stroke: #999; stroke-dasharray: 4 4; }
        .vec { stroke-width: 3; fill: none; }
        .pt  { fill: #000; }
        .lbl { font-family: sans-serif; font-size: 14px; }
        .theta { fill: none; stroke-width: 3; opacity: .6; }
      </style>
    </defs>

    <!-- Points -->
    <circle cx="180" cy="260" r="6" class="pt"/>
    <circle cx="360" cy="180" r="6" class="pt"/>
    <circle cx="540" cy="120" r="6" class="pt"/>

    <!-- Labels for points -->
    <text x="168" y="280" class="lbl">A (quadril)</text>
    <text x="342" y="170" class="lbl">B (joelho)</text>
    <text x="548" y="140" class="lbl">C (tornozelo)</text>

    <!-- Vectors BA and BC -->
    <line x1="360" y1="180" x2="180" y2="260" class="vec" stroke="#1f77b4" marker-end="url(#arrow)"/>
    <line x1="360" y1="180" x2="540" y2="120" class="vec" stroke="#2ca02c" marker-end="url(#arrow)"/>
    <text x="250" y="210" class="lbl" fill="#1f77b4">BA</text>
    <text x="460" y="155" class="lbl" fill="#2ca02c">BC</text>

    <!-- Angle arc at B -->
    <path d="M 450 150 A 90 90 0 0 0 300 210" class="theta" stroke="#d62728"/>
    <text x="390" y="150" class="lbl" fill="#d62728">θ = ∠ABC</text>

    <!-- Formula -->
    <text x="60" y="330" class="lbl">cos(θ) = (BA · BC) / (‖BA‖ · ‖BC‖)</text>
  </svg>
  <figcaption style="text-align:center; font-family: sans-serif; font-size: 14px;">
    Geometria do cálculo do ângulo no vértice B, com vetores BA e BC e representação do arco correspondente ao ângulo θ.
  </figcaption>
</figure>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A lógica temporal organiza-se como uma máquina de estados finitos que integra percepção instantânea e persistência temporal. Quando o estimador de pose não identifica marcos corporais, o sistema entra no estado de ausência, inicia um cronômetro e, se a condição persiste por pelo menos cinco segundos, emite uma notificação de ausência por meio do Telegram. Quando a pose é detectada, o sistema reinicia a contagem de ausência e transita para estados de presença. Se a classificação identifica o estado em pé, o sistema inicia um segundo cronômetro e, ao completar cinco segundos de persistência ininterrupta nesse estado, emite uma notificação de presença em pé. Se a classificação não indica em pé, o sistema mantém presença sem disparo e reinicia a contagem para o caso de o estado em pé voltar a ocorrer, o que preserva a capacidade de gerar novos alertas em eventos subsequentes.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
As credenciais de acesso (<code>TELEGRAM_TOKEN</code> e <code>TELEGRAM_CHAT_ID</code>) são obtidas por variáveis de ambiente, carregadas via <code>python-dotenv</code>. O envio de notificações utiliza requisições HTTP <code>POST</code> por meio da biblioteca <code>requests</code>, com verificação de códigos de retorno e registro de sucesso/erro no console. Esse mecanismo permite coleta e auditoria de resultados em canal externo ao programa. O sistema exibe o rótulo do estado com contorno escuro e preenchimento claro para legibilidade, além da sobreposição dos marcos anatômicos, fornecendo explicabilidade visual. O encerramento pela tecla <code>q</code> é intuitivo. 
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A solução utiliza OpenCV, executa em Ubuntu com webcam do laboratório, é produzida pela própria equipe, emprega bibliotecas notórias e gratuitas, realiza cálculos em tempo real sobre o vídeo, adota cabeçalhos padronizados nos arquivos, apresenta (ou permite configurar) o título da janela com programa e equipe, adere ao Contexto e Cenário de Aplicação e possibilita uso por usuários leigos com coleta de resultados via Telegram. Os eventos relevantes (persistência de “Em pé” e ausência prolongada) são enviados ao Telegram, funcionando como log externo com data e hora. A janela de vídeo atua como evidência visual em tempo real durante testes e demonstrações, enquanto o histórico do chat pode ser exportado para documentação.
</p>


<h3 id="lista">Lista de arquivos: dos código-fonte, imagens, vídeos, e arquivos auxiliares.</h3>
<h3 id="-parte-1">Escrever uma análise técnica.</h3>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  sobre o grau de atendimento do sistema desenvolvido em relação ao contexto/cenário escolhido: 
 apresentar medidas ou métricas objetivas (numéricas) e/ou qualitativas,
resultantes do desempenho; análise e conclusões. 
</p>

<h2 id="lab">Laboratório Experimental</h2>
<h3 id="Roteiro">Roteiro do laboratório Experimental</h3>
<h3 id="model">Análise dos Resultados do Teste de Campo TCS</h3>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  apresentar os resultados
de forma organizada, listando detalhadamente os experimentos realizados e
os critérios de avaliação. Analisar também as médias dos alunos, e as
opiniões subjetivas.
</p>
	
<h2 id="conclus-o">Conclusões</h2>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
Nesta seção a equipe deverá comentar se os objetivos propostos na
introdução e a modelagem do trabalho foram atingidos ou não, baseados nos
exemplos apresentados. Deverá ainda comentar sobre os pontos positivos e
negativos e da implementação.
</p>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A implementação atende aos requisitos funcionais e acadêmicos do SPV: utiliza OpenCV e bibliotecas reconhecidas, executa em Ubuntu com webcam, realiza cálculos em tempo real fundamentados na pose estimada, oferece visualização explicável e notifica condições críticas coerentes com o cenário de aplicação. Para conformidade formal plena, padroniza-se o cabeçalho dos arquivos e configura-se o título da janela com programa e equipe.
</p>

<h2>Referências Bibliográficas</h2>
  <p>
    OPENCV. Feature Detection and Description. Disponível em: https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html. Acesso em: 25 jul. 2025.
  </p>
  <p>
    OPENCV. Understanding Features. Disponível em: https://docs.opencv.org/4.x/df/d54/tutorial_py_features_meaning.html. Acesso em: 25 jul. 2025.
  </p>
  <p>
    OPENCV. Harris Corner Detection. Disponível em: https://docs.opencv.org/4.x/dc/d0d/tutorial_py_features_harris.html. Acesso em: 25 jul. 2025.
  </p>
    <p>
    OPENCV. Shi-Tomasi Corner Detector & Good Features to Track. Disponível em: https://docs.opencv.org/4.x/d4/d8c/tutorial_py_shi_tomasi.html. Acesso em: 25 jul. 2025.
  </p>
  <p>
    OPENCV. Introduction to SIFT (Scale-Invariant Feature Transform). Disponível em: https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html. Acesso em: 25 jul. 2025.
  </p>
  <p>
    OPENCV. Feature Matching + Homography to find Objects. Disponível em: https://docs.opencv.org/4.x/d1/de0/tutorial_py_feature_homography.html. Acesso em: 25 jul. 2025.
  </p>
</section>

<h2>Anexo</h2>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
deverá apresentar os códigos e todos arquivos
</p>
  
