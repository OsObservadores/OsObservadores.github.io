<head>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>

<h1 id="relat-rio-laborat-rio-1">Relatório Laboratório 5 - Features - Extração de Características</h1>
<p>Jorge Luiz Pinto Junior  - RA: 11058715 - CEO</p>
<p>Marcos Baldrigue Andrade - RA: 11201921777 - CFO - Financeiro</p>
<p>Guilherme Eduardo Pereira - RA: 11201720498 - CPO - Desenvolvimento</p>
<p>Data de realização do experimento: 23/07/2025</p>
<p>Data de publicação do relatório: 28/07/2025</p>
<h2 id="objetivo-o">Objetivo</h2>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
</p> 

<h2 id="intro-o">Introdução</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
</p>



<h2 id="procedimentos-experimentais">Procedimentos Experimentais</h2>

<h3 id="-parte-1">PARTE 1 - Estudo da teoria.</h3>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
Foram apresentados os seguintes links com o conteúdo a ser estudado para execução do experimento: <strong>Link 1:</strong> <a href="https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html" target="_blank">
    Página Sumário
  </a>
 , <strong>Link 2:</strong> <a href="https://docs.opencv.org/4.x/df/d54/tutorial_py_features_meaning.html" target="_blank">
        Understanding Features
  </a>
 ,<strong>Link 3:</strong> <a href="https://docs.opencv.org/4.x/dc/d0d/tutorial_py_features_harris.html" target="_blank">
        Harris Corner Detection
  </a>
  ,<strong>Link 4:</strong> <a href="https://docs.opencv.org/4.x/d4/d8c/tutorial_py_shi_tomasi.html" target="_blank">
        Shi-Tomasi Corner Detector & Good Features to Track
  </a>
    ,<strong>Link 5:</strong> <a href="https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html" target="_blank">
        Introduction to SIFT (Scale-Invariant Feature Transform)
  </a>.
  </p> 

<h3 id="-parte-2">PARTE 2 - SIFT.</h3>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  O presente experimento reproduz e adapta técnicas de processamento de imagem utilizando a biblioteca OpenCV, com foco na detecção e correspondência de características visuais entre diferentes imagens. O objetivo central é aplicar o método conhecido como Feature Matching aliado à transformação de homografia, de modo a identificar e localizar um objeto presente em duas perspectivas distintas. A implementação é dividida em duas etapas complementares: uma primeira baseada em imagens previamente gravadas e uma segunda em fluxo contínuo obtido por câmeras de uma configuração estereoscópica calibrada.
</p>

<h3 id="-a-leitura-de-imagem-em-arquivo">(A) Elaboração de um programa OpenCV para extração de características e correspondê-las em imagens diferentes.</h3>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  Na primeira etapa (Parte A), desenvolve-se um programa seguindo o tutorial oficial do OpenCV intitulado <a href="https://docs.opencv.org/4.x/d1/de0/tutorial_py_feature_homography.html" target="_blank"> Feature Matching + Homography to find Objects</a>. O programa lê duas imagens salvas, cada uma contendo o mesmo objeto, porém em posições e ângulos diferentes. Em seguida, ele aplica algoritmos de detecção e descrição de pontos-chave, calcula correspondências entre as imagens e realiza a homografia para projetar a posição do objeto em uma das imagens sobre a outra, exibindo ao final as correspondências encontradas de forma visual, conforme demonstrado abaixo.
</p>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  Abaixo são apresentados dois programas. O primeiro programa foi utilizado para capturar as imagens que foram processadas para extração de características. O segundo programa foi utilizado para processar as imagens, este extrai as características e realiza a correspondência entre elas. As imagens capturadas são mostradas por último nesta sessão e o resultado do processamento é apresentado na sessão de <strong>Análise dos Resultados</strong>.
</p>

<pre><code class="language-python">

capture_imagens_teste.py

import numpy as np
import cv2
import time
import os

# Cria os diretórios se não existirem
os.makedirs('./data/stereo1', exist_ok=True)
os.makedirs('./data/stereo2', exist_ok=True)

print("Checking the right and left camera IDs:")
print("Press (y) if IDs are correct and (n) to swap the IDs")
print("Press enter to start the process >> ")
input()

# Check for left and right camera IDs
CamL_id = 2
CamR_id = 0

CamL= cv2.VideoCapture(CamL_id)
CamR= cv2.VideoCapture(CamR_id)

for i in range(100):
    retL, frameL= CamL.read()
    retR, frameR= CamR.read()

cv2.imshow('imgL',frameL)
cv2.imshow('imgR',frameR)

if cv2.waitKey(0) & 0xFF == ord('y') or cv2.waitKey(0) & 0xFF == ord('Y'):
    CamL_id = 2
    CamR_id = 0
    print("Camera IDs maintained")

elif cv2.waitKey(0) & 0xFF == ord('n') or cv2.waitKey(0) & 0xFF == ord('N'):
    CamL_id = 0
    CamR_id = 2
    print("Camera IDs swapped")
else:
    print("Wrong input response")
    exit(-1)
CamR.release()
CamL.release()

CamL= cv2.VideoCapture(CamL_id)
CamR= cv2.VideoCapture(CamR_id)
output_path = "./data/"

start = time.time()
T = 10
count = 0

while True:
    retR, frameR = CamR.read()
    retL, frameL = CamL.read()
    
    if not retL or not retR:
        print("Erro ao capturar imagens das câmeras.")
        break

    # Exibe as imagens
    cv2.imshow('imgR', frameR)
    cv2.imshow('imgL', frameL)

    # Converte para tons de cinza
    grayR = cv2.cvtColor(frameR, cv2.COLOR_BGR2GRAY)
    grayL = cv2.cvtColor(frameL, cv2.COLOR_BGR2GRAY)

    # Procura o padrão de tabuleiro 9x6
    retR_corners, cornersR = cv2.findChessboardCorners(grayR, (8, 6), None)
    retL_corners, cornersL = cv2.findChessboardCorners(grayL, (8, 6), None)

    key = cv2.waitKey(1) & 0xFF

    if key == 27:  # Tecla ESC
        print("Encerrando a captura.")
        break

    elif key == 32:  # Tecla espaço (ASCII 32)
        if retR_corners and retL_corners:
            count += 1
            cv2.imwrite(output_path + f'stereo1/imgteste.png', frameR)
            cv2.imwrite(output_path + f'stereo2/imgteste.png', frameL)
            print(f"Imagem {count} salva com sucesso!")
        else:
            print("Tabuleiro de xadrez não detectado em ambas as imagens.")


# Release the Cameras
CamR.release()
CamL.release()
cv2.destroyAllWindows()
</code></pre>

<pre><code class="language-python">

SIFTfeatures.py

import numpy as np
import cv2 as cv
from matplotlib import pyplot as plt
 
MIN_MATCH_COUNT = 10
 
img1 = cv.imread('img1.png', cv.IMREAD_GRAYSCALE)          # queryImage
img2 = cv.imread('img1_2.png', cv.IMREAD_GRAYSCALE) # trainImage
 
# Initiate SIFT detector
sift = cv.SIFT_create()
 
# find the keypoints and descriptors with SIFT
kp1, des1 = sift.detectAndCompute(img1,None)
kp2, des2 = sift.detectAndCompute(img2,None)
 
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
search_params = dict(checks = 50)
 
flann = cv.FlannBasedMatcher(index_params, search_params)
 
matches = flann.knnMatch(des1,des2,k=2)
 
# store all the good matches as per Lowe's ratio test.
good = []
for m,n in matches:
    if m.distance < 0.7*n.distance:
        good.append(m)

######################################################################

if len(good)>MIN_MATCH_COUNT:
    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)
    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)
 
    M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC,5.0)
    matchesMask = mask.ravel().tolist()
 
    h,w = img1.shape
    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)
    dst = cv.perspectiveTransform(pts,M)
 
    img2 = cv.polylines(img2,[np.int32(dst)],True,255,3, cv.LINE_AA)
 
else:
    print( "Not enough matches are found - {}/{}".format(len(good), MIN_MATCH_COUNT) )
    matchesMask = None
    
##########################################################################

draw_params = dict(matchColor = (0,255,0), # draw matches in green color
                   singlePointColor = None,
                   matchesMask = matchesMask, # draw only inliers
                   flags = 2)
 
img3 = cv.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)
 
plt.imshow(img3, 'gray'),plt.show()

</code></pre>

<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab5/Parte A/img1.png" alt="letf1" width="400" height="200">
  <img src="/Lab5/Parte A/img1_2.png" alt="right1" width="400" height="200">
</div>
<div style="display: flex; justify-content: center">
  <figcaption>Figura 1 - Imagens capturadas pela câmera estereoscópica.</figcaption>
</div>


<h3 id="-b-leitura-de-v-deo-em-arquivo">(B) Elaboração de um programa OpenCV para extração de características envolvendo uma câmera estereoscópica calibrada.</h3>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
 Na segunda etapa (Parte B), modifica-se o programa anterior para que ele opere em tempo real. Em vez de trabalhar com arquivos de imagem, o código realiza leituras simultâneas de duas webcams pertencentes a uma câmera estereoscópica calibrada. O processamento de Feature Matching e homografia é mantido, mas agora é aplicado continuamente aos quadros capturados, permitindo visualizar ao vivo as correspondências entre as duas perspectivas. Essa adaptação demonstra a aplicabilidade do método em sistemas de captura dinâmica e em contextos onde a cena se altera em tempo real. 
</p>


  



<h2 id="analise-dos-resultados">Análise dos Resultados</h2>

<h3 id="-parte-2">PARTE 2 - SIFT.</h3>

<h3 id="-a-leitura-de-imagem-em-arquivo">(A) Elaboração de um programa OpenCV para extração de características e correspondê-las em imagens diferentes.</h3>

<h3 id="-b-leitura-de-v-deo-em-arquivo">(B) Elaboração de um programa OpenCV para extração de características envolvendo uma câmera estereoscópica calibrada.</h3>

<h3 id="-parte-2">PARTE 3 - Hough Transform.</h3>

<h3 id="-a-leitura-de-imagem-em-arquivo">(C) Elaboração de um programa OpenCV que realiza "Hough Transform".</h3>

<h3 id="-b-leitura-de-v-deo-em-arquivo">(D) Elaboração de um programa OpenCV que realiza "Hough Transform" envolvendo uma câmera estereoscópica calibrada.</h3>



<figure style="text-align: center;">
<video width="640" height="480" controls autoplay loop muted>
  <source src="Resultado_Parte_3_B/stereoR2.mp4" type="video/mp4">
</video>
<figcaption>Vídeo 2 - Câmera lado direito.</figcaption>
</figure>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  O script <strong>movie3d.py</strong> utiliza os parâmetros calculados para gerar o mapa de disparidades através das imagens estéreo geradas através das configurações de câmera estéreo. Uma imagem anáglifa para cada par de imagens estéreo é gerada e em seguida, são salvas todas as imagens anáglifas consecutivas como um vídeo. 
</p>

<figure style="text-align: center;">
<video width="640" height="480" controls autoplay loop muted>
  <source src="Resultado_Parte_3_B/parteB.mp4" type="video/mp4">
</video>
<figcaption>Vídeo 3 - Vídeo 3D gerado a partir da exibição consecutiva de imagens anáglifas.</figcaption>
</figure>


	
<h2 id="conclus-o">Conclusão</h2>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  
</p>



<h2>Referências</h2>
  <p>
    LEARNOPENCV. Making a Low-Cost Stereo Camera using OpenCV. Disponível em: https://learnopencv.com/making-a-low-cost-stereo-camera-using-opencv/#creating-a-custom-3d-video. Acesso em: 21 jul. 2025.
  </p>
  <p>
    LEARNOPENCV. Introduction to Epipolar Geometry and Stereo Vision. Disponível em: https://learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/. Acesso em: 21 jul. 2025.
  </p>
  <p>
    LEARNOPENCV. Understanding Lens Distortion. Disponível em: https://learnopencv.com/understanding-lens-distortion/. Acesso em: 21 jul. 2025.
  </p>
</section>
  
