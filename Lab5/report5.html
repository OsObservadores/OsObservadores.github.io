<head>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>

<h1 id="relat-rio-laborat-rio-1">Relatório Laboratório 5 - Features - Extração de Características</h1>
<p>Jorge Luiz Pinto Junior  - RA: 11058715 - CEO</p>
<p>Marcos Baldrigue Andrade - RA: 11201921777 - CFO - Financeiro</p>
<p>Guilherme Eduardo Pereira - RA: 11201720498 - CPO - Desenvolvimento</p>
<p>Data de realização do experimento: 23/07/2025</p>
<p>Data de publicação do relatório: 28/07/2025</p>
<h2 id="objetivo-o">Objetivo</h2>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
O objetivo deste experimento é compreender os conceitos de features e extração de características, incluindo o funcionamento das técnicas SIFT e Harris Corner Detection, bem como realizar experimentos de extração e detecção dessas características em duas imagens distintas.
</p> 

<h2 id="intro-o">Introdução</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
 A detecção e descrição de características (features) constitui um dos fundamentos da visão computacional e está presente em diversas aplicações, como reconhecimento de objetos, rastreamento e reconstrução de cenas. Uma feature representa um ponto de interesse ou um padrão localizado em uma imagem, geralmente associado a regiões com variações significativas de intensidade, como quinas, bordas e texturas. O estudo desses elementos permite que algoritmos identifiquem correspondências entre diferentes imagens e realizem tarefas avançadas, como alinhamento e mapeamento de objetos.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
Diversos métodos são empregados para detectar e descrever features. Entre os algoritmos clássicos estão o Harris Corner Detector, que identifica quinas em regiões de variação de gradiente, e o Shi-Tomasi Corner Detector, que aprimora a seleção de pontos relevantes para rastreamento. Esses métodos estabelecem as bases para técnicas mais robustas, como o SIFT (Scale-Invariant Feature Transform), que detecta e descreve características invariantes a escala e rotação, garantindo maior confiabilidade em cenários com mudanças de perspectiva ou iluminação.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A partir dessas detecções, é possível realizar a correspondência de características (Feature Matching), técnica que compara descritores de duas imagens distintas para identificar regiões equivalentes. Quando aplicada em conjunto com a homografia, essa correspondência permite mapear a posição de um objeto entre diferentes pontos de vista, possibilitando que sua localização seja projetada de uma imagem para outra. Esses conceitos e técnicas, apresentados nos tutoriais da documentação do OpenCV, fundamentam a execução do experimento e orientam a implementação dos algoritmos utilizados. 
</p>

<figure style="text-align: center;">
  <img src="featuresmatching.jpg" alt="Figura1" width="600" height="500">
  <figcaption>Figura 1 - Correspondência de features.</figcaption>
</figure>


<h2 id="procedimentos-experimentais">Procedimentos Experimentais</h2>

<h3 id="-parte-1">PARTE 1 - Estudo da teoria.</h3>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
Foram apresentados os seguintes links com o conteúdo a ser estudado para execução do experimento: <strong>Link 1:</strong> <a href="https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html" target="_blank">
    Página Sumário
  </a>
 , <strong>Link 2:</strong> <a href="https://docs.opencv.org/4.x/df/d54/tutorial_py_features_meaning.html" target="_blank">
        Understanding Features
  </a>
 ,<strong>Link 3:</strong> <a href="https://docs.opencv.org/4.x/dc/d0d/tutorial_py_features_harris.html" target="_blank">
        Harris Corner Detection
  </a>
  ,<strong>Link 4:</strong> <a href="https://docs.opencv.org/4.x/d4/d8c/tutorial_py_shi_tomasi.html" target="_blank">
        Shi-Tomasi Corner Detector & Good Features to Track
  </a>
    ,<strong>Link 5:</strong> <a href="https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html" target="_blank">
        Introduction to SIFT (Scale-Invariant Feature Transform)
  </a>.
  </p> 

<h3 id="-parte-2">PARTE 2 - SIFT.</h3>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  O presente experimento reproduz e adapta técnicas de processamento de imagem utilizando a biblioteca OpenCV, com foco na detecção e correspondência de características visuais entre diferentes imagens. O objetivo central é aplicar o método conhecido como Feature Matching aliado à transformação de homografia, de modo a identificar e localizar um objeto presente em duas perspectivas distintas. A implementação é dividida em duas etapas complementares: uma primeira baseada em imagens previamente gravadas e uma segunda em fluxo contínuo obtido por câmeras de uma configuração estereoscópica calibrada.
</p>

<h3 id="-a-leitura-de-imagem-em-arquivo">(A) Elaboração de um programa OpenCV para extração de características e correspondê-las em imagens diferentes.</h3>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  Na primeira etapa (Parte A), desenvolve-se um programa seguindo o tutorial oficial do OpenCV intitulado <a href="https://docs.opencv.org/4.x/d1/de0/tutorial_py_feature_homography.html" target="_blank"> Feature Matching + Homography to find Objects</a>. O programa lê duas imagens salvas, cada uma contendo o mesmo objeto, porém em posições e ângulos diferentes. Em seguida, ele aplica algoritmos de detecção e descrição de pontos-chave, calcula correspondências entre as imagens e realiza a homografia para projetar a posição do objeto em uma das imagens sobre a outra, exibindo ao final as correspondências encontradas de forma visual, conforme demonstrado abaixo.
</p>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  Abaixo são apresentados dois programas. O primeiro programa foi utilizado para capturar as imagens que foram processadas para extração de características. O segundo programa foi utilizado para processar as imagens, este extrai as características e realiza a correspondência entre elas. As imagens capturadas são mostradas por último nesta sessão e o resultado do processamento é apresentado na sessão de <strong>Análise dos Resultados</strong>.
</p>

<pre><code class="language-python">

capture_imagens_teste.py

import numpy as np
import cv2
import time
import os

# Cria os diretórios se não existirem
os.makedirs('./data/stereo1', exist_ok=True)
os.makedirs('./data/stereo2', exist_ok=True)

print("Checking the right and left camera IDs:")
print("Press (y) if IDs are correct and (n) to swap the IDs")
print("Press enter to start the process >> ")
input()

# Check for left and right camera IDs
CamL_id = 2
CamR_id = 0

CamL= cv2.VideoCapture(CamL_id)
CamR= cv2.VideoCapture(CamR_id)

for i in range(100):
    retL, frameL= CamL.read()
    retR, frameR= CamR.read()

cv2.imshow('imgL',frameL)
cv2.imshow('imgR',frameR)

if cv2.waitKey(0) & 0xFF == ord('y') or cv2.waitKey(0) & 0xFF == ord('Y'):
    CamL_id = 2
    CamR_id = 0
    print("Camera IDs maintained")

elif cv2.waitKey(0) & 0xFF == ord('n') or cv2.waitKey(0) & 0xFF == ord('N'):
    CamL_id = 0
    CamR_id = 2
    print("Camera IDs swapped")
else:
    print("Wrong input response")
    exit(-1)
CamR.release()
CamL.release()

CamL= cv2.VideoCapture(CamL_id)
CamR= cv2.VideoCapture(CamR_id)
output_path = "./data/"

start = time.time()
T = 10
count = 0

while True:
    retR, frameR = CamR.read()
    retL, frameL = CamL.read()
    
    if not retL or not retR:
        print("Erro ao capturar imagens das câmeras.")
        break

    # Exibe as imagens
    cv2.imshow('imgR', frameR)
    cv2.imshow('imgL', frameL)

    # Converte para tons de cinza
    grayR = cv2.cvtColor(frameR, cv2.COLOR_BGR2GRAY)
    grayL = cv2.cvtColor(frameL, cv2.COLOR_BGR2GRAY)

    # Procura o padrão de tabuleiro 9x6
    retR_corners, cornersR = cv2.findChessboardCorners(grayR, (8, 6), None)
    retL_corners, cornersL = cv2.findChessboardCorners(grayL, (8, 6), None)

    key = cv2.waitKey(1) & 0xFF

    if key == 27:  # Tecla ESC
        print("Encerrando a captura.")
        break

    elif key == 32:  # Tecla espaço (ASCII 32)
        if retR_corners and retL_corners:
            count += 1
            cv2.imwrite(output_path + f'stereo1/imgteste.png', frameR)
            cv2.imwrite(output_path + f'stereo2/imgteste.png', frameL)
            print(f"Imagem {count} salva com sucesso!")
        else:
            print("Tabuleiro de xadrez não detectado em ambas as imagens.")


# Release the Cameras
CamR.release()
CamL.release()
cv2.destroyAllWindows()
</code></pre>

<pre><code class="language-python">

SIFTfeatures.py

import numpy as np
import cv2 as cv
from matplotlib import pyplot as plt
 
MIN_MATCH_COUNT = 10
 
img1 = cv.imread('img1.png', cv.IMREAD_GRAYSCALE)          # queryImage
img2 = cv.imread('img1_2.png', cv.IMREAD_GRAYSCALE) # trainImage
 
# Initiate SIFT detector
sift = cv.SIFT_create()
 
# find the keypoints and descriptors with SIFT
kp1, des1 = sift.detectAndCompute(img1,None)
kp2, des2 = sift.detectAndCompute(img2,None)
 
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
search_params = dict(checks = 50)
 
flann = cv.FlannBasedMatcher(index_params, search_params)
 
matches = flann.knnMatch(des1,des2,k=2)
 
# store all the good matches as per Lowe's ratio test.
good = []
for m,n in matches:
    if m.distance < 0.7*n.distance:
        good.append(m)

######################################################################

if len(good)>MIN_MATCH_COUNT:
    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)
    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)
 
    M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC,5.0)
    matchesMask = mask.ravel().tolist()
 
    h,w = img1.shape
    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)
    dst = cv.perspectiveTransform(pts,M)
 
    img2 = cv.polylines(img2,[np.int32(dst)],True,255,3, cv.LINE_AA)
 
else:
    print( "Not enough matches are found - {}/{}".format(len(good), MIN_MATCH_COUNT) )
    matchesMask = None
    
##########################################################################

draw_params = dict(matchColor = (0,255,0), # draw matches in green color
                   singlePointColor = None,
                   matchesMask = matchesMask, # draw only inliers
                   flags = 2)
 
img3 = cv.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)
 
plt.imshow(img3, 'gray'),plt.show()

</code></pre>

<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab5/Parte A/img1.png" alt="letf1" width="400" height="200">
  <img src="/Lab5/Parte A/img1_2.png" alt="right1" width="400" height="200">
</div>
<div style="display: flex; justify-content: center">
  <figcaption>Figura 2 - Imagens capturadas pela câmera estereoscópica.</figcaption>
</div>


<h3 id="-b-leitura-de-v-deo-em-arquivo">(B) Elaboração de um programa OpenCV para extração de características envolvendo uma câmera estereoscópica calibrada.</h3>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
 Na segunda etapa (Parte B), modifica-se o programa anterior para que ele opere em tempo real. Em vez de trabalhar com arquivos de imagem, o código realiza leituras simultâneas de duas webcams pertencentes a uma câmera estereoscópica calibrada. O processamento de Feature Matching e homografia é mantido, mas agora é aplicado continuamente aos quadros capturados, permitindo visualizar ao vivo as correspondências entre as duas perspectivas. Essa adaptação demonstra a aplicabilidade do método em sistemas de captura dinâmica e em contextos onde a cena se altera em tempo real. O programa modificado é apresentado a seguir e os resultados e analises apresentados na sessão <strong>Análise dos Resultados</strong>.
</p>

<pre><code class="language-python">

SIFTvideo.py

import threading
import time
import numpy as np
import cv2 as cv

class CameraThread(threading.Thread):
    def __init__(self, cam_id):
        super().__init__()
        self.cap = cv.VideoCapture(cam_id)
        self.ret = False
        self.frame = None
        self.running = True
        self.lock = threading.Lock()

        for _ in range(10):  # aquecimento da câmera
            ret, frame = self.cap.read()
            if ret:
                break
            time.sleep(0.1)

    def run(self):
        while self.running:
            ret, frame = self.cap.read()
            with self.lock:
                self.ret = ret
                if ret:
                    self.frame = frame
            time.sleep(0.001)

    def get_frame(self):
        with self.lock:
            return self.ret, self.frame.copy() if self.frame is not None else None

    def stop(self):
        self.running = False
        self.cap.release()


CamL_id = 2
CamR_id = 0

camL_thread = CameraThread(CamL_id)
camR_thread = CameraThread(CamR_id)

camL_thread.start()
camR_thread.start()

fps = 10.0
#width = int(camL_thread.cap.get(cv.CAP_PROP_FRAME_WIDTH))
height = int(camL_thread.cap.get(cv.CAP_PROP_FRAME_HEIGHT))
fourcc = cv.VideoWriter_fourcc(*'mp4v')
outm = cv.VideoWriter('SIFTvideo.mp4', fourcc, fps, (1280, height))

# Inicializa o SIFT e FLANN
sift = cv.SIFT_create()
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
search_params = dict(checks=50)
flann = cv.FlannBasedMatcher(index_params, search_params)

MIN_MATCH_COUNT = 10



try:
    while True:
        retL, frameL = camL_thread.get_frame()
        retR, frameR = camR_thread.get_frame()

        if not retL or not retR:
            time.sleep(0.01)
            continue

        # Converte para escala de cinza
        grayL = cv.cvtColor(frameL, cv.COLOR_BGR2GRAY)
        grayR = cv.cvtColor(frameR, cv.COLOR_BGR2GRAY)

        # Detecta keypoints e descritores
        kp1, des1 = sift.detectAndCompute(grayL, None)
        kp2, des2 = sift.detectAndCompute(grayR, None)

        if des1 is None or des2 is None:
            continue

        # Match com FLANN
        matches = flann.knnMatch(des1, des2, k=2)

        # Lowe’s ratio test
        good = []
        for m, n in matches:
            if m.distance < 0.7 * n.distance:
                good.append(m)

        matchesMask = None

        if len(good) > MIN_MATCH_COUNT:
            src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)
            dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)

            M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC, 5.0)
            matchesMask = mask.ravel().tolist()
        else:
            matchesMask = None

        # Desenhar os matches
        draw_params = dict(matchColor=(0, 255, 0),
                           singlePointColor=None,
                           matchesMask=matchesMask,
                           flags=2)

        matched_img = cv.drawMatches(grayL, kp1, grayR, kp2, good, None, **draw_params)
		
        print(f"matched_img shape: {matched_img.shape}")
		
        outm.write(matched_img)
 
        cv.imshow("Feature Matching Real-Time", matched_img)
        
        if cv.waitKey(1) & 0xFF == ord('q'):
            break

finally:
    camL_thread.stop()
    camR_thread.stop()
    cv.destroyAllWindows()

</code></pre>

<h3 id="-parte-2">PARTE 3 - Hough Transform.</h3>

<h3 id="-a-leitura-de-imagem-em-arquivo">(C) Elaboração de um programa OpenCV que realiza "Hough Transform".</h3>

<h3 id="-b-leitura-de-v-deo-em-arquivo">(D) Elaboração de um programa OpenCV que realiza "Hough Transform" envolvendo uma câmera estereoscópica calibrada.</h3>

<h2 id="analise-dos-resultados">Análise dos Resultados</h2>

<h3 id="-parte-2">PARTE 2 - SIFT.</h3>

<h3 id="-a-leitura-de-imagem-em-arquivo">(A) Elaboração de um programa OpenCV para extração de características e correspondê-las em imagens diferentes.</h3>

<h3 id="analise-dos-resultados">Análise do programa <strong>SIFTfeatures.py</strong></h3>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
O programa realiza a detecção, descrição e correspondência de pontos de interesse entre duas imagens utilizando o algoritmo SIFT (Scale-Invariant Feature Transform) e a técnica de Feature Matching com o uso do FLANN (Fast Library for Approximate Nearest Neighbors). Ele identifica pontos-chave nas duas imagens, calcula descritores e aplica o teste de razão de Lowe para selecionar apenas as correspondências mais consistentes. Quando a quantidade de correspondências válidas é suficiente, o programa estima uma homografia entre as imagens por meio do método RANSAC, projetando o contorno da primeira imagem sobre a segunda para indicar a posição do objeto correspondente. Por fim, ele exibe uma imagem resultante mostrando as correspondências válidas com linhas verdes, destacando visualmente a relação entre os pontos de interesse das duas imagens. O resultado do processamento das duas imagens é apresentado a seguir:
</p>

<figure style="text-align: center;">
  <img src="/Lab5/Parte A/Figure_1.png" alt="Figura1" width="600" height="500">
  <figcaption>Figura 3 - Extração e correspondências de características.</figcaption>
</figure>

<h3 id="-b-leitura-de-v-deo-em-arquivo">(B) Elaboração de um programa OpenCV para extração de características envolvendo uma câmera estereoscópica calibrada.</h3>

<h3 id="analise-dos-resultados">Análise do programa <strong>SIFTvideo.py</strong></h3>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
O programa realiza a captura simultânea de imagens em tempo real a partir de duas webcams pertencentes a uma câmera estereoscópica, utilizando threads independentes para cada dispositivo a fim de garantir a leitura contínua e sincronizada. Em cada iteração, o sistema converte os quadros capturados para escala de cinza, detecta pontos-chave e extrai descritores com o algoritmo SIFT, e em seguida executa o processo de correspondência entre as duas imagens por meio do FLANN Matcher, aplicando o teste de razão de Lowe para filtrar as melhores correspondências. Quando o número mínimo de matches é atingido, o programa calcula a homografia entre as duas visões, permitindo alinhar as correspondências válidas e destacá-las graficamente. A imagem resultante, mostrando os pontos de interesse correspondidos, é exibida em uma janela em tempo real e simultaneamente gravada em um arquivo de vídeo, possibilitando análise posterior do processo de Feature Matching dinâmico. O vídeo obtido pelo programa é apresentado a seguir:
</p>

<figure style="text-align: center;">
<video width="1280" height="480" controls autoplay loop muted>
  <source src="/Lab5/Parte B/SIFTvideo2.mp4" type="video/mp4">
</video>
<figcaption>Vídeo 1 - Resultados da dectecção e correspondência de características em vídeo utilizando uma câmera estereoscópica.</figcaption>
</figure>

<h3 id="-a-leitura-de-imagem-em-arquivo">(C) Elaboração de um programa OpenCV que realiza "Hough Transform" e cria linhas.</h3>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
Enquanto a técnica de Feature Matching utilizando o FLANN localiza correspondências entre pontos-chave nas imagens, a Transformada de Hough (Hough Transform) é utilizada para detectar linhas retas, permitindo destacar estruturas lineares presentes na cena. Abaixo, é possível observar diferentes exemplos da aplicação da Transformada de Hough:
</p>

<figure style="text-align: center;">
  <img src="/Lab5/Parte C/saida_linhas.jpg" alt="Figura4" width="600" height="500">
  <figcaption>Figura 4 - poucas linhas na estrada</figcaption>
</figure>
<figure style="text-align: center;">
  <img src="/Lab5/Parte C/saida_linhas_.jpg" alt="Figura5" width="600" height="500">
  <figcaption>Figura 5 - muitas linhas na estrada</figcaption>
</figure>
<figure style="text-align: center;">
  <img src="/Lab5/Parte C/saida_linhas_estrada.jpg" alt="Figura6" width="600" height="500">
  <figcaption>Figura 6 - linhas na estrada</figcaption>
</figure>
<figure style="text-align: center;">
  <img src="/Lab5/Parte C/saida_linhas_igreja.jpg" alt="Figura7" width="600" height="500">
  <figcaption>Figura 7 - linhas em Notre Dame</figcaption>
</figure>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
É importante observar que as Figuras 4 e 5 se referem à mesma imagem base, porém com configurações diferentes para a detecção de linhas. Na Figura 4, há um número reduzido de linhas detectadas, o que pode resultar em perda de informações importantes, como faixas da pista. Já na Figura 5, o aumento da sensibilidade leva à detecção de linhas em excesso, o que pode gerar ruído visual.
</p>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
Essa técnica tem aplicação prática em sistemas de direção autônoma. A identificação das linhas da estrada permite que veículos automatizados se mantenham corretamente na faixa e tomem decisões de navegação com maior segurança, reduzindo o risco de acidentes.
</p>

<h3 id="-a-leitura-de-imagem-em-arquivo">(C) Elaboração de um programa OpenCV que realiza "Hough Transform" e cria circunferências.</h3>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
A Transformada de Hough também pode ser configurada para identificar formas circulares em imagens. Essa capacidade é demonstrada nas imagens a seguir:
</p>

<figure style="text-align: center;">
  <img src="/Lab5/Parte C/saida_circulos.jpg" alt="Figura8" width="600" height="500">
  <figcaption>Figura 8 - circulos na placa</figcaption>
</figure>
<figure style="text-align: center;">
  <img src="/Lab5/Parte C/saida_circulos-igreja.jpg" alt="Figura9" width="600" height="500">
  <figcaption>Figura 9 - circulos da igreja</figcaption>
</figure>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
Assim como na detecção de linhas, a identificação automática de circunferências pode ser integrada a veículos autônomos. Um exemplo prático seria a detecção de sinais luminosos — como os faróis de trânsito — onde o programa pode identificar as luzes vermelha, amarela e verde, auxiliando o carro a parar ou seguir em cruzamentos com mais segurança.
</p>



<h3 id="-b-leitura-de-v-deo-em-arquivo">(D) Elaboração de um programa OpenCV que realiza "Hough Transform" envolvendo uma câmera estereoscópica calibrada.</h3>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
O programa que utiliza a Transformada de Hough para detecção de linhas e circunferências foi aplicado em imagens capturadas por uma câmera estéreo previamente calibrada. O resultado pode ser observado no vídeo a seguir:
</p>
<figure style="text-align: center;">
<video width="640" height="480" controls autoplay loop muted>
  <source src="/Lab5/Parte D/HoughVideo.mp4" type="video/mp4">
</video>
<figcaption>Vídeo 2 - Hough transform com câmera estereoscópica calibrada.</figcaption>
</figure>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
No vídeo, é possível perceber a eficácia da Transformada de Hough na identificação de estruturas geométricas na cena. As linhas detectadas seguem contornos reais do ambiente, como o encosto da cadeira e a estrutura metálica que a sustenta. Ao fundo, o algoritmo também identifica corretamente as linhas do teto, demonstrando a sensibilidade da técnica mesmo para objetos em diferentes profundidades.
</p>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
No caso das circunferências, a detecção depende dos valores de raio mínimo e máximo configurados no algoritmo. Isso é evidente ao observar o símbolo da UFABC: quando o símbolo está fora da faixa de raio especificada — por estar muito próximo ou muito distante da câmera — a circunferência correspondente não é detectada. No entanto, quando o símbolo está a uma distância adequada, o algoritmo reconhece com sucesso sua forma circular.
</p>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
Essa análise reforça a importância da calibração da câmera: ao corrigir distorções ópticas e fornecer parâmetros intrínsecos precisos, a calibração melhora significativamente a qualidade da detecção de formas. Com isso, o sistema pode ser aplicado em diversas áreas, como robótica, navegação autônoma e análise de cenas tridimensionais.
</p>

<h3 id="analise-dos-resultados">Análise de como as técnicas de Detecção de Features pode ser utilizada no Trabalho T1</h3>




	
<h2 id="conclus-o">Conclusão</h2>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  
</p>



<h2>Referências</h2>
  <p>
    OPENCV. Feature Detection and Description. Disponível em: https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html. Acesso em: 25 jul. 2025.
  </p>
  <p>
    OPENCV. Understanding Features. Disponível em: https://docs.opencv.org/4.x/df/d54/tutorial_py_features_meaning.html. Acesso em: 25 jul. 2025.
  </p>
  <p>
    OPENCV. Harris Corner Detection. Disponível em: https://docs.opencv.org/4.x/dc/d0d/tutorial_py_features_harris.html. Acesso em: 25 jul. 2025.
  </p>
    <p>
    OPENCV. Shi-Tomasi Corner Detector & Good Features to Track. Disponível em: https://docs.opencv.org/4.x/d4/d8c/tutorial_py_shi_tomasi.html. Acesso em: 25 jul. 2025.
  </p>
  <p>
    OPENCV. Introduction to SIFT (Scale-Invariant Feature Transform). Disponível em: https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html. Acesso em: 25 jul. 2025.
  </p>
  <p>
    OPENCV. Feature Matching + Homography to find Objects. Disponível em: https://docs.opencv.org/4.x/d1/de0/tutorial_py_feature_homography.html. Acesso em: 25 jul. 2025.
  </p>
</section>
  
