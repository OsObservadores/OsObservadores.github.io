<head>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>

<h1 id="relat-rio-laborat-rio-1">Relatório Laboratório 4 - Mapa de profundidade</h1>
<p>Jorge Luiz Pinto Junior  - RA: 11058715 - CEO</p>
<p>Marcos Baldrigue Andrade - RA: 11201921777 - CFO - Financeiro</p>
<p>Guilherme Eduardo Pereira - RA: 11201720498 - CPO - Desenvolvimento</p>
<p>Data de realização do experimento: 21/07/2025</p>
<p>Data de publicação do relatório: 04/08/2025</p>
<h2 id="objetivo-o">Objetivo</h2>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
O presente experimento tem como objetivo compreender e aplicar os conceitos de visão computacional estéreo, com ênfase na geração e interpretação de mapas de disparidade e profundidade. Busca-se, especificamente, analisar o processo de retificação de imagens capturadas por um sistema de câmeras estéreo e realizar um experimento prático de estimativa da distância de objetos com base no mapa de disparidade. Por meio dessa abordagem, pretende-se demonstrar a viabilidade da utilização de técnicas de estéreo visão para a obtenção de informações tridimensionais do ambiente a partir de imagens bidimensionais.
</p> 

<h2 id="intro-o">Introdução</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  A percepção de profundidade é uma habilidade fundamental do sistema visual humano, viabilizada pela visão estéreo, que utiliza a diferença entre as imagens captadas por cada olho para inferir a distância dos objetos. Essa capacidade, conhecida como estereopsia, inspira o desenvolvimento de sistemas de visão computacional que buscam reconstruir informações tridimensionais a partir de imagens bidimensionais. No contexto da visão computacional, a geometria epipolar e a triangulação constituem a base teórica para a estimativa de profundidade a partir de pares de imagens estéreo.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  Quando um ponto do espaço tridimensional é projetado em uma única imagem, perde-se a informação de profundidade devido à natureza da projeção planar. Para recuperar essa informação, torna-se necessário capturar o mesmo ponto a partir de dois pontos de vista distintos, permitindo a aplicação da triangulação. A correta correspondência entre os pontos nas duas imagens é facilitada pelo uso da geometria epipolar, que restringe a busca do ponto correspondente a uma única linha epipolar na imagem adjacente. Com a calibração das câmeras e o conhecimento da configuração espacial entre elas, é possível aplicar algoritmos de disparidade e gerar mapas de profundidade precisos. A utilização de métodos como o StereoSGBM no OpenCV exemplifica a aplicação prática desses conceitos, permitindo a reconstrução tridimensional densa de cenas reais.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
O conceito de mapa de profundidade fundamenta-se na relação inversa entre a disparidade observada e a distância dos objetos capturados por uma câmera estéreo. Quando o sistema processa pares de imagens retificadas, cada pixel gera um valor de disparidade que corresponde ao deslocamento horizontal entre as posições correlatas nas duas imagens. A partir dessa disparidade, calcula-se a profundidade 
Z
Z usando a fórmula clássica 
Z=f×Bd
Z=
d
f×B
	​

, onde 
f
f representa a distância focal em pixels, 
B
B indica a linha base (distância entre as câmeras) e 
d
d é a disparidade 
learnopencv.com
GeeksforGeeks
. Em cenários práticos, como diferenças entre os parâmetros ópticos ou imprecisões na medição da linha base, o valor de profundidade é estimado experimentalmente, determinando-se um coeficiente 
M
M que relaciona profundidade e inverso da disparidade por meio de um ajuste de mínimos quadrados 
learnopencv.com
. Dessa forma, o mapa de disparidade é convertido em um mapa de profundidade, viabilizando a percepção tridimensional do ambiente.
</p>


<h2 id="procedimentos-experimentais">Procedimentos Experimentais</h2>

<h3 id="-parte-1">PARTE 1 - Estudo da teoria.</h3>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
Foram apresentados os seguintes links com o conteúdo a ser estudado para execução do experimento: <strong>Link 1:</strong> <a href="https://learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/" target="_blank">
    Estudo da geometria epipolar com duas cameras/imagens
  </a>, <strong>Link 2:</strong> <a href="https://docs.opencv.org/4.x/dd/d53/tutorial_py_depthmap.html target="_blank">
    Estudo da formação do Mapa de Profundidade com Par de Imagens Estéreo
  </a>
 e <strong>Link 3:</strong> <a href="https://learnopencv.com/depth-perception-using-stereo-camera-python-c/" target="_blank">
        Estudo do Mapa de Disparidade com OpenCV com Câmeras
  </a>.
  </p> 

<h3 id="-parte-2">PARTE 2 - Construção de uma câmera estereoscópica simples.</h3>

<h3 id="-a-leitura-de-imagem-em-arquivo">(i) Calibração estéreo e arquivo <strong>params_py.xml</strong></h3>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  Para realização desta etapa, foram fornecidas imagens tiradas de uma câmera estereoscópica (junção de duas câmeras iguais) desconhecida. As imagens para cada câmera foram geradas ao mesmo tempo. E através destas, foram calibradas as duas câmeras (lado direito e lado esquerdo). A seguir são apresentadas as imagens fornecidas, onde a imagem a esquerda provém da primeira câmera e a imagem a direita da segunda câmera.
</p>


<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  O script a seguir é o código 'capture_images.py' que realiza a captura das imagens com as duas câmeras.
</p>

<pre><code class="language-python">
# Importa bibliotecas necessárias
import numpy as np              # Biblioteca para cálculos numéricos (não usada diretamente aqui)
import cv2                      # OpenCV: biblioteca para processamento de imagem e vídeo
import time                     # Para medir tempo e controlar intervalos

# Exibe instruções para o usuário
print("Checking the right and left camera IDs:")
print("Press (y) if IDs are correct and (n) to swap the IDs")
print("Press enter to start the process >> ")
input()  # Aguarda o usuário pressionar Enter

# Define os IDs das câmeras (pode variar de acordo com o PC)
CamL_id = 0                     # ID da câmera da esquerda
CamR_id = 2                     # ID da câmera da direita

# Inicializa as câmeras
CamL = cv2.VideoCapture(CamL_id)
CamR = cv2.VideoCapture(CamR_id)

# Lê 100 frames de cada câmera (isso ajuda a "aquecer" a câmera)
for i in range(100):
    retL, frameL = CamL.read()
    retR, frameR = CamR.read()

# Mostra os últimos frames capturados para o usuário verificar se as câmeras estão corretas
cv2.imshow('imgL', frameL)  # Imagem da câmera da esquerda
cv2.imshow('imgR', frameR)  # Imagem da câmera da direita

# O bloco abaixo foi comentado, mas serviria para o usuário confirmar ou trocar os IDs das câmeras
# if cv2.waitKey(0) & 0xFF == ord('y'):
#     CamL_id = 2
#     CamR_id = 0
#     print("Camera IDs maintained")
# elif cv2.waitKey(0) & 0xFF == ord('n'):
#     CamL_id = 0
#     CamR_id = 2
#     print("Camera IDs swapped")
# else:
#     print("Wrong input response")
#     exit(-1)

# Libera (fecha) as câmeras
CamR.release()
CamL.release()

# Reabre as câmeras com os IDs definidos anteriormente
CamL = cv2.VideoCapture(CamL_id)
CamR = cv2.VideoCapture(CamR_id)

# Define o caminho para salvar as imagens capturadas
output_path = "./data"

# Inicia um cronômetro
start = time.time()

T = 1000           # Tempo entre capturas, em milissegundos (esse valor será ajustado no loop)
count = 0          # Contador de imagens salvas

# Loop principal
while True:
    # Calcula o tempo restante até a próxima tentativa de salvar uma imagem
    timer = T - int(time.time() - start)

    # Lê um frame de cada câmera
    retR, frameR = CamR.read()
    retL, frameL = CamL.read()

    # Cria uma cópia da imagem da esquerda para mostrar o tempo restante
    img1_temp = frameL.copy()
    cv2.putText(img1_temp, "%r" % timer, (50, 50), 1, 5, (55, 0, 0), 5)

    # Mostra as imagens das duas câmeras na tela
    cv2.imshow('imgR', frameR)
    cv2.imshow('imgL', img1_temp)

    # Converte as imagens para tons de cinza (necessário para detecção de padrão de xadrez)
    grayR = cv2.cvtColor(frameR, cv2.COLOR_BGR2GRAY)
    grayL = cv2.cvtColor(frameL, cv2.COLOR_BGR2GRAY)

    # Procura por um padrão de tabuleiro de xadrez 8x6 nas duas imagens
    retR, cornersR = cv2.findChessboardCorners(grayR, (8, 6), None)
    retL, cornersL = cv2.findChessboardCorners(grayL, (8, 6), None)

    # Se o padrão for encontrado nas duas imagens E o tempo chegar a 0, salva as imagens
    if (retR == True) and (retL == True) and timer <= 0:
        count += 1  # Atualiza o número da imagem
        # Salva a imagem da direita
        cv2.imwrite('./data/stereoR/img%d.png' % count, frameR)
        # Salva a imagem da esquerda
        cv2.imwrite('./data/stereoL/img%d.png' % count, frameL)
        print("Foi")  # Confirma que as imagens foram salvas

    # Se o tempo chegou a 0, reinicia o cronômetro
    if timer <= 0:
        start = time.time()

    # Se a tecla ESC (código 27) for pressionada, sai do loop
    if cv2.waitKey(1) & 0xFF == 27:
        print("Closing the cameras!")
        break

# Após sair do loop, libera as câmeras e fecha todas as janelas
CamR.release()
CamL.release()
cv2.destroyAllWindows()
</code></pre>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  O código a seguir é o 'calibrate.py' que vai calibrar em estéreo com parâmetros intrínsecos fixos, retificação estéreo e por fim o mapeamento que obtém o par de imagens estéreo retificadas e sem distorções.</p>

<pre><code class="language-python">
# Importa as bibliotecas necessárias
import numpy as np                # Usada para trabalhar com arrays e matrizes
import cv2                        # OpenCV: biblioteca para processamento de imagens e vídeo
from tqdm import tqdm             # Biblioteca para mostrar uma barra de progresso no terminal

# Define os caminhos para as imagens capturadas pelas câmeras esquerda e direita
pathL = "./data/stereoL/"
pathR = "./data/stereoR/"

print("Extracting image coordinates of respective 3D pattern ....\n")

# Critério de parada para melhorar a precisão dos cantos detectados (refinamento dos cantos do tabuleiro)
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)

# Prepara um array com as posições 3D esperadas dos cantos do tabuleiro de xadrez
# Nesse caso, 8 colunas x 6 linhas de quadrados (48 pontos), com z=0 (superfície plana)
objp = np.zeros((8*6,3), np.float32)
objp[:,:2] = np.mgrid[0:8,0:6].T.reshape(-1,2)

# Listas para armazenar:
img_ptsL = []  # Cantos detectados nas imagens da câmera esquerda
img_ptsR = []  # Cantos detectados nas imagens da câmera direita
obj_pts = []   # Posições 3D correspondentes a esses cantos

# Loop para processar as imagens de 1 até 13
for i in tqdm(range(1,14)):  # tqdm mostra uma barra de progresso bonitinha
    # Lê as imagens coloridas e em escala de cinza das duas câmeras
    imgL = cv2.imread('./data/stereoL/img%d.png'%i)
    imgR = cv2.imread('./data/stereoR/img%d.png'%i)
    imgL_gray = cv2.imread('./data/stereoL/img%d.png'%i, 0)
    imgR_gray = cv2.imread('./data/stereoR/img%d.png'%i, 0)

    # Faz uma cópia para desenhar os cantos depois
    outputL = imgL.copy()
    outputR = imgR.copy()

    # Tenta encontrar o tabuleiro de xadrez nas imagens
    retR, cornersR = cv2.findChessboardCorners(outputR, (8,6), None)
    retL, cornersL = cv2.findChessboardCorners(outputL, (8,6), None)

    # Se o padrão for detectado nas duas imagens
    if retR and retL:
        # Adiciona os pontos 3D (iguais para todas as imagens) na lista
        obj_pts.append(objp)

        # Melhora a precisão dos cantos encontrados
        cv2.cornerSubPix(imgR_gray, cornersR, (11,11), (-1,-1), criteria)
        cv2.cornerSubPix(imgL_gray, cornersL, (11,11), (-1,-1), criteria)

        # Desenha os cantos detectados para visualização
        cv2.drawChessboardCorners(outputR, (8,6), cornersR, retR)
        cv2.drawChessboardCorners(outputL, (8,6), cornersL, retL)
        cv2.imshow('cornersR', outputR)
        cv2.imshow('cornersL', outputL)
        cv2.waitKey(0)  # Espera uma tecla para continuar (útil para ver os cantos)

        # Salva os pontos 2D das imagens nas listas
        img_ptsL.append(cornersL)
        img_ptsR.append(cornersR)

# Calibração da câmera esquerda
print("Calculating left camera parameters ... ")
retL, mtxL, distL, rvecsL, tvecsL = cv2.calibrateCamera(obj_pts, img_ptsL, imgL_gray.shape[::-1], None, None)

# Calcula a matriz de câmera ajustada para melhor visualização (evita distorções)
hL, wL = imgL_gray.shape[:2]
new_mtxL, roiL = cv2.getOptimalNewCameraMatrix(mtxL, distL, (wL, hL), 1, (wL, hL))

# Calibração da câmera direita
print("Calculating right camera parameters ... ")
retR, mtxR, distR, rvecsR, tvecsR = cv2.calibrateCamera(obj_pts, img_ptsR, imgR_gray.shape[::-1], None, None)

# Também calcula a matriz ajustada da câmera direita
hR, wR = imgR_gray.shape[:2]
new_mtxR, roiR = cv2.getOptimalNewCameraMatrix(mtxR, distR, (wR, hR), 1, (wR, hR))

# Calibração estéreo entre as duas câmeras
print("Stereo calibration .....")

flags = 0
flags |= cv2.CALIB_FIX_INTRINSIC  # Fixa os parâmetros internos (já calibrados individualmente)
# Assim, só serão calculadas:
# - Rotação (Rot), Translação (Trns), Matriz Essencial (Emat) e Matriz Fundamental (Fmat)

# Critérios de parada para a calibração estéreo
criteria_stereo = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)

# Calcula os parâmetros entre as duas câmeras (posição relativa, etc.)
retS, new_mtxL, distL, new_mtxR, distR, Rot, Trns, Emat, Fmat = cv2.stereoCalibrate(
    obj_pts, img_ptsL, img_ptsR,
    new_mtxL, distL, new_mtxR, distR,
    imgL_gray.shape[::-1], criteria_stereo, flags
)

# Retificação estéreo: alinha as imagens para facilitar a correspondência de pixels
rectify_scale = 1  # Se 0, corta as bordas para eliminar áreas inúteis; se 1, mantém tudo

# Gera as matrizes de retificação e projeção, e a matriz Q para reconstrução 3D
rect_l, rect_r, proj_mat_l, proj_mat_r, Q, roiL, roiR = cv2.stereoRectify(
    new_mtxL, distL, new_mtxR, distR,
    imgL_gray.shape[::-1], Rot, Trns,
    rectify_scale, (0, 0)
)

# Gera os mapas de remapeamento para retificar e corrigir distorções nas imagens
# Esses mapas dizem como transformar cada pixel da imagem original para sua nova posição
Left_Stereo_Map = cv2.initUndistortRectifyMap(
    new_mtxL, distL, rect_l, proj_mat_l,
    imgL_gray.shape[::-1], cv2.CV_16SC2
)

Right_Stereo_Map = cv2.initUndistortRectifyMap(
    new_mtxR, distR, rect_r, proj_mat_r,
    imgR_gray.shape[::-1], cv2.CV_16SC2
)

# Salva os parâmetros de retificação em um arquivo XML
print("Saving parameters ......")
cv_file = cv2.FileStorage("data/params_py.xml", cv2.FILE_STORAGE_WRITE)

# Escreve os mapas de retificação no arquivo
cv_file.write("Left_Stereo_Map_x", Left_Stereo_Map[0])
cv_file.write("Left_Stereo_Map_y", Left_Stereo_Map[1])
cv_file.write("Right_Stereo_Map_x", Right_Stereo_Map[0])
cv_file.write("Right_Stereo_Map_y", Right_Stereo_Map[1])

# Fecha o arquivo
cv_file.release()

</code></pre>

<h3 id="-b-leitura-de-v-deo-em-arquivo">(ii) Executar o algoritmo Block Matching do OpenCV, sintonizando seus parâmetros.</h3>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  O código a seguir é o “disparity_params_gui.py” que vai montar o mapa de disparidade utilizando o "params_py.xml"
	</p>

<pre><code class="language-python">
import numpy as np 
import cv2


# Check for left and right camera IDs
# These values can change depending on the system
CamL_id = 0 # Camera ID for left camera
CamR_id = 2 # Camera ID for right camera

CamL= cv2.VideoCapture(CamL_id)
CamR= cv2.VideoCapture(CamR_id)

# Reading the mapping values for stereo image rectification
cv_file = cv2.FileStorage("./data/params_py.xml", cv2.FILE_STORAGE_READ)
Left_Stereo_Map_x = cv_file.getNode("Left_Stereo_Map_x").mat()
Left_Stereo_Map_y = cv_file.getNode("Left_Stereo_Map_y").mat()
Right_Stereo_Map_x = cv_file.getNode("Right_Stereo_Map_x").mat()
Right_Stereo_Map_y = cv_file.getNode("Right_Stereo_Map_y").mat()
cv_file.release()

def nothing(x):
    pass

cv2.namedWindow('disp',cv2.WINDOW_NORMAL)
cv2.resizeWindow('disp',600,600)

cv2.createTrackbar('numDisparities','disp',1,17,nothing)
cv2.createTrackbar('blockSize','disp',5,50,nothing)
cv2.createTrackbar('preFilterType','disp',1,1,nothing)
cv2.createTrackbar('preFilterSize','disp',2,25,nothing)
cv2.createTrackbar('preFilterCap','disp',5,62,nothing)
cv2.createTrackbar('textureThreshold','disp',10,100,nothing)
cv2.createTrackbar('uniquenessRatio','disp',15,100,nothing)
cv2.createTrackbar('speckleRange','disp',0,100,nothing)
cv2.createTrackbar('speckleWindowSize','disp',3,25,nothing)
cv2.createTrackbar('disp12MaxDiff','disp',5,25,nothing)
cv2.createTrackbar('minDisparity','disp',5,25,nothing)

# Creating an object of StereoBM algorithm
stereo = cv2.StereoBM_create()

while True:

	# Capturing and storing left and right camera images
	retL, imgL= CamL.read()
	retR, imgR= CamR.read()
	
	# Proceed only if the frames have been captured
	if retL and retR:
		imgR_gray = cv2.cvtColor(imgR,cv2.COLOR_BGR2GRAY)
		imgL_gray = cv2.cvtColor(imgL,cv2.COLOR_BGR2GRAY)

		# Applying stereo image rectification on the left image
		Left_nice= cv2.remap(imgL_gray,
							Left_Stereo_Map_x,
							Left_Stereo_Map_y,
							cv2.INTER_LANCZOS4,
							cv2.BORDER_CONSTANT,
							0)
		
		# Applying stereo image rectification on the right image
		Right_nice= cv2.remap(imgR_gray,
							Right_Stereo_Map_x,
							Right_Stereo_Map_y,
							cv2.INTER_LANCZOS4,
							cv2.BORDER_CONSTANT,
							0)

		# Updating the parameters based on the trackbar positions
		numDisparities = cv2.getTrackbarPos('numDisparities','disp')*16
		blockSize = cv2.getTrackbarPos('blockSize','disp')*2 + 5
		preFilterType = cv2.getTrackbarPos('preFilterType','disp')
		preFilterSize = cv2.getTrackbarPos('preFilterSize','disp')*2 + 5
		preFilterCap = cv2.getTrackbarPos('preFilterCap','disp')
		textureThreshold = cv2.getTrackbarPos('textureThreshold','disp')
		uniquenessRatio = cv2.getTrackbarPos('uniquenessRatio','disp')
		speckleRange = cv2.getTrackbarPos('speckleRange','disp')
		speckleWindowSize = cv2.getTrackbarPos('speckleWindowSize','disp')*2
		disp12MaxDiff = cv2.getTrackbarPos('disp12MaxDiff','disp')
		minDisparity = cv2.getTrackbarPos('minDisparity','disp')
		
		# Setting the updated parameters before computing disparity map
		stereo.setNumDisparities(numDisparities)
		stereo.setBlockSize(blockSize)
		stereo.setPreFilterType(preFilterType)
		stereo.setPreFilterSize(preFilterSize)
		stereo.setPreFilterCap(preFilterCap)
		stereo.setTextureThreshold(textureThreshold)
		stereo.setUniquenessRatio(uniquenessRatio)
		stereo.setSpeckleRange(speckleRange)
		stereo.setSpeckleWindowSize(speckleWindowSize)
		stereo.setDisp12MaxDiff(disp12MaxDiff)
		stereo.setMinDisparity(minDisparity)

		# Calculating disparity using the StereoBM algorithm
		disparity = stereo.compute(Left_nice,Right_nice)
		# NOTE: compute returns a 16bit signed single channel image,
		# CV_16S containing a disparity map scaled by 16. Hence it 
		# is essential to convert it to CV_32F and scale it down 16 times.

		# Converting to float32 
		disparity = disparity.astype(np.float32)

		# Scaling down the disparity values and normalizing them 
		disparity = (disparity/16.0 - minDisparity)/numDisparities

		# Displaying the disparity map
		cv2.imshow("disp",disparity)

		# Close window using esc key
		if cv2.waitKey(1) == 27:
			break
	
	else:
		CamL= cv2.VideoCapture(CamL_id)
		CamR= cv2.VideoCapture(CamR_id)

print("Saving depth estimation paraeters ......")

cv_file = cv2.FileStorage("./data/depth_estmation_params_py_test2.xml", cv2.FILE_STORAGE_WRITE)
cv_file.write("numDisparities",numDisparities)
cv_file.write("blockSize",blockSize)
cv_file.write("preFilterType",preFilterType)
cv_file.write("preFilterSize",preFilterSize)
cv_file.write("preFilterCap",preFilterCap)
cv_file.write("textureThreshold",textureThreshold)
cv_file.write("uniquenessRatio",uniquenessRatio)
cv_file.write("speckleRange",speckleRange)
cv_file.write("speckleWindowSize",speckleWindowSize)
cv_file.write("disp12MaxDiff",disp12MaxDiff)
cv_file.write("minDisparity",minDisparity)
cv_file.write("M",39.075)
cv_file.release()
</code></pre>
<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab4/data_lab4/data/Imagem colada (2).png" alt="letf01" width="1080" height="640">
</div>

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
	Com isso, é gerado o arquivo "depth_estimation_params_py.xml", que deve ser usado no mapa de profundidade.
</p>

<h3 id="-b-leitura-de-v-deo-em-arquivo">(iii) Obter o mapa de profundidade. </h3>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
	Neste caso é utilizado o programa “disparity2depth_calib.py”. Este programa foi adaptado para leitura do arquivo “params_py.xml”, com os parâmetros de calibração da câmera estéreo. Neste caso, foi utilizado o mapa de disparidade do arquivo “depth_estmation_params_py.xml”.
</p>

<pre><code class="language-python">
import cv2
import numpy as np

# --- 1. Lê os parâmetros de calibração estéreo ---
cv_file = cv2.FileStorage("./data/params_py.xml", cv2.FILE_STORAGE_READ)
Left_Stereo_Map_x = cv_file.getNode("Left_Stereo_Map_x").mat()
Left_Stereo_Map_y = cv_file.getNode("Left_Stereo_Map_y").mat()
Right_Stereo_Map_x = cv_file.getNode("Right_Stereo_Map_x").mat()
Right_Stereo_Map_y = cv_file.getNode("Right_Stereo_Map_y").mat()
cv_file.release()

# --- 2. Lê o mapa de disparidade já salvo ---
cv_file = cv2.FileStorage("./data/depth_estmation_params_py.xml", cv2.FILE_STORAGE_READ)
disparity = cv_file.getNode("disparity").mat()
cv_file.release()

# --- 3. Amostras de calibração prática (Exemplo real) ---
# Substitua pelos seus valores medidos na prática
# Exemplo: [(disparity, depth_in_cm), ...]
samples = [
    (30.5, 100),  # objeto a 1m
    (21.0, 150),  # objeto a 1.5m
    (15.0, 200),  # objeto a 2m
    (12.0, 250),  # objeto a 2.5m
]

# --- 4. Prepara os dados para resolver: depth = M0*(1/disparity) + M1 ---
N = len(samples)
coeff = np.zeros((N, 2), dtype=np.float32)
z = np.zeros((N, 1), dtype=np.float32)

for i, (disp, depth) in enumerate(samples):
    coeff[i, 0] = 1.0 / disp
    coeff[i, 1] = 1.0
    z[i, 0] = depth

# --- 5. Resolve para M ---
ret, M = cv2.solve(coeff, z, flags=cv2.DECOMP_QR)
M0, M1 = M[0, 0], M[1, 0]
print(f"Coeficientes calibrados: M0 = {M0:.4f}, M1 = {M1:.4f}")

# --- 6. Calcula o mapa de profundidade real ---
# Evita divisões por zero ou valores negativos
disp_safe = np.copy(disparity)
disp_safe[disp_safe <= 0.0] = np.nan  # ignora valores inválidos

# Aplica a equação depth = M0*(1/disp) + M1
depth_map = M0 * (1.0 / disp_safe) + M1

# --- 7. Salva o mapa de profundidade real no mesmo XML ---
fs_write = cv2.FileStorage("./data/depth_estmation_params_py_2.xml", cv2.FILE_STORAGE_APPEND)
fs_write.write("depth", depth_map)
fs_write.release()
print("✅ Mapa de profundidade salvo em depth_estmation_params_py.xml")
</code></pre>

<h3 id="-b-leitura-de-v-deo-em-arquivo">(iv)  Realizar medidas de distância seguindo as orientações indicadas na seção “Obstacle avoidance system” . </h3>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
	Neste caso é utilizado o programa “obstacle_avoidance.py”. Este programa foi adaptado para leitura do arquivo “params_py.xml”, e do arquivo “depth_estmation_params_py.xml” para estipular distância de objetos a partir dos parametros obtidos nos códigos anteriores.
</p>
<pre><code class="language-python">
import numpy as np 
import cv2


# Check for left and right camera IDs
# These values can change depending on the system
CamL_id = 0 # Camera ID for left camera
CamR_id = 2 # Camera ID for right camera


CamL= cv2.VideoCapture(CamL_id)
CamR= cv2.VideoCapture(CamR_id)

# Reading the mapping values for stereo image rectification
cv_file = cv2.FileStorage("./data/params_py_jorge.xml", cv2.FILE_STORAGE_READ)
Left_Stereo_Map_x = cv_file.getNode("Left_Stereo_Map_x").mat()
Left_Stereo_Map_y = cv_file.getNode("Left_Stereo_Map_y").mat()
Right_Stereo_Map_x = cv_file.getNode("Right_Stereo_Map_x").mat()
Right_Stereo_Map_y = cv_file.getNode("Right_Stereo_Map_y").mat()
cv_file.release()

disparity = None
depth_map = None

# These parameters can vary according to the setup
max_depth = 400 # maximum distance the setup can measure (in cm)
min_depth = 50 # minimum distance the setup can measure (in cm)
depth_thresh = 100.0 # Threshold for SAFE distance (in cm)

# Reading the stored the StereoBM parameters
cv_file = cv2.FileStorage("../data/depth_estmation_params_py_test_jorge.xml", cv2.FILE_STORAGE_READ)
numDisparities = int(cv_file.getNode("numDisparities").real())
blockSize = int(cv_file.getNode("blockSize").real())
preFilterType = int(cv_file.getNode("preFilterType").real())
preFilterSize = int(cv_file.getNode("preFilterSize").real())
preFilterCap = int(cv_file.getNode("preFilterCap").real())
textureThreshold = int(cv_file.getNode("textureThreshold").real())
uniquenessRatio = int(cv_file.getNode("uniquenessRatio").real())
speckleRange = int(cv_file.getNode("speckleRange").real())
speckleWindowSize = int(cv_file.getNode("speckleWindowSize").real())
disp12MaxDiff = int(cv_file.getNode("disp12MaxDiff").real())
minDisparity = int(cv_file.getNode("minDisparity").real())
M = cv_file.getNode("M").real()
cv_file.release()

# mouse callback function
def mouse_click(event,x,y,flags,param):
	global Z
	if event == cv2.EVENT_LBUTTONDBLCLK:
		print("Distance = %.2f cm"%depth_map[y,x])	


cv2.namedWindow('disp',cv2.WINDOW_NORMAL)
cv2.resizeWindow('disp',600,600)
cv2.setMouseCallback('disp',mouse_click)

output_canvas = None

# Creating an object of StereoBM algorithm
stereo = cv2.StereoBM_create()

def obstacle_avoid():

	# Mask to segment regions with depth less than threshold
	mask = cv2.inRange(depth_map,10,depth_thresh)

	# Check if a significantly large obstacle is present and filter out smaller noisy regions
	if np.sum(mask)/255.0 > 0.01*mask.shape[0]*mask.shape[1]:

		# Contour detection 
		contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
		cnts = sorted(contours, key=cv2.contourArea, reverse=True)
		
		# Check if detected contour is significantly large (to avoid multiple tiny regions)
		if cv2.contourArea(cnts[0]) > 0.01*mask.shape[0]*mask.shape[1]:

			x,y,w,h = cv2.boundingRect(cnts[0])

			# finding average depth of region represented by the largest contour 
			mask2 = np.zeros_like(mask)
			cv2.drawContours(mask2, cnts, 0, (255), -1)

			# Calculating the average depth of the object closer than the safe distance
			depth_mean, _ = cv2.meanStdDev(depth_map, mask=mask2)
			
			# Display warning text
			cv2.putText(output_canvas, "WARNING !", (x+5,y-40), 1, 2, (0,0,255), 2, 2)
			cv2.putText(output_canvas, "Object at", (x+5,y), 1, 2, (100,10,25), 2, 2)
			cv2.putText(output_canvas, "%.2f cm"%depth_mean, (x+5,y+40), 1, 2, (100,10,25), 2, 2)

	else:
		cv2.putText(output_canvas, "SAFE!", (100,100),1,3,(0,255,0),2,3)

	cv2.imshow('output_canvas',output_canvas)
	

while True:
	retR, imgR= CamR.read()
	retL, imgL= CamL.read()
	
	if retL and retR:
		
		output_canvas = imgL.copy()

		imgR_gray = cv2.cvtColor(imgR,cv2.COLOR_BGR2GRAY)
		imgL_gray = cv2.cvtColor(imgL,cv2.COLOR_BGR2GRAY)

		# Applying stereo image rectification on the left image
		Left_nice= cv2.remap(imgL_gray,
							Left_Stereo_Map_x,
							Left_Stereo_Map_y,
							cv2.INTER_LANCZOS4,
							cv2.BORDER_CONSTANT,
							0)
		
		# Applying stereo image rectification on the right image
		Right_nice= cv2.remap(imgR_gray,
							Right_Stereo_Map_x,
							Right_Stereo_Map_y,
							cv2.INTER_LANCZOS4,
							cv2.BORDER_CONSTANT,
							0)

		# Setting the updated parameters before computing disparity map
		stereo.setNumDisparities(numDisparities)
		stereo.setBlockSize(blockSize)
		stereo.setPreFilterType(preFilterType)
# Garantir preFilterSize ímpar e dentro do intervalo permitido
        if preFilterSize < 5:
            preFilterSize = 5
        elif preFilterSize > 255:
            preFilterSize = 255

        if preFilterSize % 2 == 0:
            preFilterSize += 1  # Se for par, deixa ímpar adicionando 1
		stereo.setPreFilterSize(preFilterSize)
		stereo.setPreFilterCap(preFilterCap)
		stereo.setTextureThreshold(textureThreshold)
		stereo.setUniquenessRatio(uniquenessRatio)
		stereo.setSpeckleRange(speckleRange)
		stereo.setSpeckleWindowSize(speckleWindowSize)
		stereo.setDisp12MaxDiff(disp12MaxDiff)
		stereo.setMinDisparity(minDisparity)

		# Calculating disparity using the StereoBM algorithm
		disparity = stereo.compute(Left_nice,Right_nice)
		# NOTE: compute returns a 16bit signed single channel image,
		# CV_16S containing a disparity map scaled by 16. Hence it 
		# is essential to convert it to CV_16S and scale it down 16 times.

		# Converting to float32 
		disparity = disparity.astype(np.float32)

		# Normalizing the disparity map
		disparity = (disparity/16.0 - minDisparity)/numDisparities
		
		depth_map = M/(disparity) # for depth in (cm)

		mask_temp = cv2.inRange(depth_map,min_depth,max_depth)
		depth_map = cv2.bitwise_and(depth_map,depth_map,mask=mask_temp)

		obstacle_avoid()
		
		cv2.resizeWindow("disp",700,700)
		cv2.imshow("disp",disparity)

		if cv2.waitKey(1) == 27:
			break
	
	else:
		CamL= cv2.VideoCapture(CamL_id)
		CamR= cv2.VideoCapture(CamR_id)
</code></pre>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
	A imagem a seguir foi registrada durante a execução do código acima:
</p>

<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab4/data_lab4/data/Imagem colada.png" alt="letf01" width="640" height="480">
</div>

<h3 id="-b-leitura-de-v-deo-em-arquivo">(v) As medidas de distância realizadas deverão ser colocadas numa tabela. </h3>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
	As medidas de distância realizadas deverão ser colocadas numa tabela, e comparadas com a distância real. Calcule o erro, e faça uma análise dos resultados das medições bem como da influencia dos parametros.
</p>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
  Para realização desta etapa, foram fornecidas imagens tiradas de uma câmera estereoscópica (junção de duas câmeras iguais) desconhecida. As imagens para cada câmera foram geradas ao mesmo tempo utilizando o código 'capture_images.py'. E através destas, foram calibradas as duas câmeras (lado direito e lado esquerdo) utilizando o código 'calibrate.py'. A seguir são apresentadas as imagens fornecidas, onde a imagem a esquerda provém da primeira câmera e a imagem a direita da segunda câmera.
</p>

<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab4/data_lab4/data/stereoL/img1.png" alt="letf01" width="400" height="200">
  <img src="/Lab4/data_lab4/data/stereoR/img1.png" alt="right01" width="400" height="200">
</div>
<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab4/data_lab4/data/stereoL/img2.png" alt="letf01" width="400" height="200">
  <img src="/Lab4/data_lab4/data/stereoR/img2.png" alt="right01" width="400" height="200">
</div>
<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab4/data_lab4/data/stereoL/img3.png" alt="letf01" width="400" height="200">
  <img src="/Lab4/data_lab4/data/stereoR/img3.png" alt="right01" width="400" height="200">
</div>
<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab4/data_lab4/data/stereoL/img4.png" alt="letf01" width="400" height="200">
  <img src="/Lab4/data_lab4/data/stereoR/img4.png" alt="right01" width="400" height="200">
</div>
<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab4/data_lab4/data/stereoL/img5.png" alt="letf01" width="400" height="200">
  <img src="/Lab4/data_lab4/data/stereoR/img5.png" alt="right01" width="400" height="200">
</div>
<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab4/data_lab4/data/stereoL/img6.png" alt="letf01" width="400" height="200">
  <img src="/Lab4/data_lab4/data/stereoR/img6.png" alt="right01" width="400" height="200">
</div>
<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab4/data_lab4/data/stereoL/img7.png" alt="letf01" width="400" height="200">
  <img src="/Lab4/data_lab4/data/stereoR/img7.png" alt="right01" width="400" height="200">
</div>
<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab4/data_lab4/data/stereoL/img8.png" alt="letf01" width="400" height="200">
  <img src="/Lab4/data_lab4/data/stereoR/img8.png" alt="right01" width="400" height="200">
</div>
<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab4/data_lab4/data/stereoL/img9.png" alt="letf01" width="400" height="200">
  <img src="/Lab4/data_lab4/data/stereoR/img9.png" alt="right01" width="400" height="200">
</div>
<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab4/data_lab4/data/stereoL/img10.png" alt="letf01" width="400" height="200">
  <img src="/Lab4/data_lab4/data/stereoR/img10.png" alt="right01" width="400" height="200">
</div>
<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab4/data_lab4/data/stereoL/img11.png" alt="letf01" width="400" height="200">
  <img src="/Lab4/data_lab4/data/stereoR/img11.png" alt="right01" width="400" height="200">
</div>
<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab4/data_lab4/data/stereoL/img12.png" alt="letf01" width="400" height="200">
  <img src="/Lab4/data_lab4/data/stereoR/img12.png" alt="right01" width="400" height="200">
</div>
<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab4/data_lab4/data/stereoL/img13.png" alt="letf01" width="400" height="200">
  <img src="/Lab4/data_lab4/data/stereoR/img13.png" alt="right01" width="400" height="200">
</div>
<div style="display: flex; justify-content: center; gap: 5px;">
  <img src="/Lab4/data_lab4/data/stereoL/img14.png" alt="letf01" width="400" height="200">
  <img src="/Lab4/data_lab4/data/stereoR/img14.png" alt="right01" width="400" height="200">
</div>
<div style="display: flex; justify-content: center">
  <figcaption>Figura 1 - Imagens capturadas para calibração.</figcaption>
</div>
  

<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
Inicialmente, realiza-se a calibração individual de cada câmera por meio do método de calibração OpenCV. Em seguida, determina-se a transformação entre as duas câmeras configuradas no sistema estéreo. Com os parâmetros obtidos, emprega-se o método <strong>stereoCalibrate</strong> para calcular as transformações necessárias à retificação estéreo. A partir disso, utiliza-se o método <strong>initUndistortRectifyMap</strong> para gerar o mapeamento que corrige distorções e prepara as imagens para retificação. Por fim, aplica-se esse mapeamento às imagens originais, obtendo-se um par estéreo retificado e livre de distorções.
</p>
<h3 id="-b-leitura-de-v-deo-em-arquivo">Método para obter o par de imagens estéreo:</h3>


	
<h2 id="conclus-o">Conclusão</h2>
<p style="text-align: justify; line-height: 1.5; text-indent: 2em;">
</p>

<h2>Referências</h2>
  <p>
	  LEARNOPENCV. Making a Low-Cost Stereo Camera using OpenCV. Disponível em: https://learnopencv.com/making-a-low-cost-stereo-camera-using-opencv/#creating-a-custom-3d-video. Acesso em: 21 jul. 2025.
  </p>
  <p>
	  LEARNOPENCV. Introduction to Epipolar Geometry and Stereo Vision. Disponível em: https://learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/. Acesso em: 21 jul. 2025.
  </p>
  <p>
	  LEARNOPENCV. Understanding Lens Distortion. Disponível em: https://learnopencv.com/understanding-lens-distortion/. Acesso em: 21 jul. 2025.
  </p>
  <p>
	  LEARNOPENCV. Stereo Camera Depth Estimation With OpenCV (Python/C++). Disponível em: https://learnopencv.com/depth-perception-using-stereo-camera-python-c/. Acesso em: 21 jul. 2025.
  </p>
  <p>
	  LEARNOPENCV. Depth Map from Stereo Images. Disponível em: https://docs.opencv.org/4.x/dd/d53/tutorial_py_depthmap.html. Acesso em: 21 jul. 2025.
  </p>
  <p>
	  LOOP, C.; ZHANG, Z. Computing Rectifying Homographies for Stereo Vision. In: IEEE Conference on Computer Vision and Pattern Recognition, 1999.
  </p>
  <p>
	  LEARNOPENCV. Geometry of Image Formation. Disponível em: https://learnopencv.com/geometry-of-image-formation/. Acesso em: 21 jul. 2025.
  </p>
</section>
  
